{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevan.matovic@onetrust.com/.pyenv/versions/3.10.11/envs/ds/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/Users/stevan.matovic@onetrust.com/.pyenv/versions/3.10.11/envs/ds/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/Users/stevan.matovic@onetrust.com/.pyenv/versions/3.10.11/envs/ds/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/Users/stevan.matovic@onetrust.com/.pyenv/versions/3.10.11/envs/ds/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/Users/stevan.matovic@onetrust.com/.pyenv/versions/3.10.11/envs/ds/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on mps\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstevan-matovic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240325_234831-pakm67xc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/pakm67xc' target=\"_blank\">apricot-deluge-11</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/pakm67xc' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/pakm67xc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 73.23056888580322 seconds, Average Loss: 7.639720380306244\n",
      "Epoch 2 finished in 65.41478681564331 seconds, Average Loss: 7.149856885274251\n",
      "Epoch 3 finished in 67.92625713348389 seconds, Average Loss: 6.979749242464702\n",
      "Epoch 4 finished in 59.66105008125305 seconds, Average Loss: 6.888481259346008\n",
      "Epoch 5 finished in 57.16447424888611 seconds, Average Loss: 6.830951015154521\n",
      "Epoch 6 finished in 56.95068717002869 seconds, Average Loss: 6.792366723219554\n",
      "Epoch 7 finished in 56.857964754104614 seconds, Average Loss: 6.768590688705444\n",
      "Epoch 8 finished in 56.52297401428223 seconds, Average Loss: 6.7470338344573975\n",
      "Epoch 9 finished in 56.642690896987915 seconds, Average Loss: 6.733247558275859\n",
      "Epoch 10 finished in 56.65328288078308 seconds, Average Loss: 6.714428742726644\n",
      "Epoch 11 finished in 56.401756048202515 seconds, Average Loss: 6.701370457808177\n",
      "Epoch 12 finished in 56.55287504196167 seconds, Average Loss: 6.691389342149098\n",
      "Epoch 13 finished in 56.443639039993286 seconds, Average Loss: 6.6855834523836775\n",
      "Epoch 14 finished in 56.41944718360901 seconds, Average Loss: 6.673102776209514\n",
      "Epoch 15 finished in 56.493674993515015 seconds, Average Loss: 6.666189213593801\n",
      "Epoch 16 finished in 56.37682795524597 seconds, Average Loss: 6.658192495505015\n",
      "Epoch 17 finished in 56.27835178375244 seconds, Average Loss: 6.652191599210103\n",
      "Epoch 18 finished in 56.33529186248779 seconds, Average Loss: 6.643426477909088\n",
      "Epoch 19 finished in 56.48409867286682 seconds, Average Loss: 6.6354203422864275\n",
      "Epoch 20 finished in 56.37304997444153 seconds, Average Loss: 6.631268322467804\n",
      "Epoch 21 finished in 56.429474115371704 seconds, Average Loss: 6.6257913907368975\n",
      "Epoch 22 finished in 56.41673493385315 seconds, Average Loss: 6.620454708735148\n",
      "Epoch 23 finished in 56.421761989593506 seconds, Average Loss: 6.619634807109833\n",
      "Epoch 24 finished in 56.31790900230408 seconds, Average Loss: 6.61335963010788\n",
      "Epoch 25 finished in 56.33658695220947 seconds, Average Loss: 6.610658347606659\n",
      "Epoch 26 finished in 56.37970590591431 seconds, Average Loss: 6.608123401800792\n",
      "Epoch 27 finished in 56.64019298553467 seconds, Average Loss: 6.603125552336375\n",
      "Epoch 28 finished in 57.41998291015625 seconds, Average Loss: 6.601997911930084\n",
      "Epoch 29 finished in 56.52644395828247 seconds, Average Loss: 6.598590850830078\n",
      "Epoch 30 finished in 58.00014591217041 seconds, Average Loss: 6.596102515856425\n",
      "Epoch 31 finished in 56.80368113517761 seconds, Average Loss: 6.5937511920928955\n",
      "Epoch 32 finished in 56.97047996520996 seconds, Average Loss: 6.5931549072265625\n",
      "Epoch 33 finished in 57.23430013656616 seconds, Average Loss: 6.5909750660260515\n",
      "Epoch 34 finished in 57.7866370677948 seconds, Average Loss: 6.586351792017619\n",
      "Epoch 35 finished in 57.39758110046387 seconds, Average Loss: 6.585843086242676\n",
      "Epoch 36 finished in 57.14721894264221 seconds, Average Loss: 6.585913916428884\n",
      "Epoch 37 finished in 57.833731174468994 seconds, Average Loss: 6.583732783794403\n",
      "Epoch 38 finished in 57.89600706100464 seconds, Average Loss: 6.582654535770416\n",
      "Epoch 39 finished in 59.0763680934906 seconds, Average Loss: 6.579004247983296\n",
      "Epoch 40 finished in 58.32351016998291 seconds, Average Loss: 6.578586379686992\n",
      "Epoch 41 finished in 58.1823308467865 seconds, Average Loss: 6.574739515781403\n",
      "Epoch 42 finished in 59.160987854003906 seconds, Average Loss: 6.577280680338542\n",
      "Epoch 43 finished in 59.22223496437073 seconds, Average Loss: 6.574664215246837\n",
      "Epoch 44 finished in 57.74547624588013 seconds, Average Loss: 6.574460367361705\n",
      "Epoch 45 finished in 56.48816108703613 seconds, Average Loss: 6.570789615313212\n",
      "Epoch 46 finished in 56.90785098075867 seconds, Average Loss: 6.5705246925354\n",
      "Epoch 47 finished in 56.525200843811035 seconds, Average Loss: 6.571406006813049\n",
      "Epoch 48 finished in 57.81453490257263 seconds, Average Loss: 6.570097585519155\n",
      "Epoch 49 finished in 57.744909048080444 seconds, Average Loss: 6.56892192363739\n",
      "Epoch 50 finished in 56.76269292831421 seconds, Average Loss: 6.566100100676219\n",
      "Epoch 51 finished in 57.242398262023926 seconds, Average Loss: 6.566269357999166\n",
      "Epoch 52 finished in 57.173588037490845 seconds, Average Loss: 6.565851628780365\n",
      "Epoch 53 finished in 57.10339879989624 seconds, Average Loss: 6.566567321618398\n",
      "Epoch 54 finished in 57.565513134002686 seconds, Average Loss: 6.564233044783275\n",
      "Epoch 55 finished in 58.32038712501526 seconds, Average Loss: 6.563840250174205\n",
      "Epoch 56 finished in 57.954543113708496 seconds, Average Loss: 6.562221050262451\n",
      "Epoch 57 finished in 58.28359413146973 seconds, Average Loss: 6.563681781291962\n",
      "Epoch 58 finished in 56.47394609451294 seconds, Average Loss: 6.5591597358385725\n",
      "Epoch 59 finished in 56.661746978759766 seconds, Average Loss: 6.561132470766704\n",
      "Epoch 60 finished in 56.437209129333496 seconds, Average Loss: 6.559005618095398\n",
      "Epoch 61 finished in 56.42433404922485 seconds, Average Loss: 6.55814262231191\n",
      "Epoch 62 finished in 56.53878903388977 seconds, Average Loss: 6.557931840419769\n",
      "Epoch 63 finished in 56.364062786102295 seconds, Average Loss: 6.556077678998311\n",
      "Epoch 64 finished in 56.46168398857117 seconds, Average Loss: 6.554566303888957\n",
      "Epoch 65 finished in 56.93253302574158 seconds, Average Loss: 6.55409707625707\n",
      "Epoch 66 finished in 56.85429382324219 seconds, Average Loss: 6.554017285505931\n",
      "Epoch 67 finished in 56.67054891586304 seconds, Average Loss: 6.55414076646169\n",
      "Epoch 68 finished in 56.444985151290894 seconds, Average Loss: 6.5521034598350525\n",
      "Epoch 69 finished in 56.49368405342102 seconds, Average Loss: 6.551583826541901\n",
      "Epoch 70 finished in 57.17778563499451 seconds, Average Loss: 6.553478201230367\n",
      "Epoch 71 finished in 56.561861991882324 seconds, Average Loss: 6.551026841004689\n",
      "Epoch 72 finished in 56.54956102371216 seconds, Average Loss: 6.552431762218475\n",
      "Epoch 73 finished in 56.5124568939209 seconds, Average Loss: 6.550094147523244\n",
      "Epoch 74 finished in 56.61430883407593 seconds, Average Loss: 6.551461478074391\n",
      "Epoch 75 finished in 58.3065071105957 seconds, Average Loss: 6.549019833405812\n",
      "Epoch 76 finished in 58.876412868499756 seconds, Average Loss: 6.550255060195923\n",
      "Epoch 77 finished in 57.696869134902954 seconds, Average Loss: 6.546660343805949\n",
      "Epoch 78 finished in 57.91826796531677 seconds, Average Loss: 6.54862368106842\n",
      "Epoch 79 finished in 57.5147819519043 seconds, Average Loss: 6.547648072242737\n",
      "Epoch 80 finished in 57.35782217979431 seconds, Average Loss: 6.547877748807271\n",
      "Epoch 81 finished in 58.19434976577759 seconds, Average Loss: 6.545295635859172\n",
      "Epoch 82 finished in 57.02898573875427 seconds, Average Loss: 6.547453860441844\n",
      "Epoch 83 finished in 56.30579400062561 seconds, Average Loss: 6.543111940224965\n",
      "Epoch 84 finished in 56.35965323448181 seconds, Average Loss: 6.5444061160087585\n",
      "Epoch 85 finished in 56.37930226325989 seconds, Average Loss: 6.543778121471405\n",
      "Epoch 86 finished in 56.39994692802429 seconds, Average Loss: 6.543296992778778\n",
      "Epoch 87 finished in 56.401896238327026 seconds, Average Loss: 6.54309465487798\n",
      "Epoch 88 finished in 56.525575160980225 seconds, Average Loss: 6.542881806691487\n",
      "Epoch 89 finished in 56.5263831615448 seconds, Average Loss: 6.542696575323741\n",
      "Epoch 90 finished in 57.1882758140564 seconds, Average Loss: 6.54374365011851\n",
      "Epoch 91 finished in 57.854835987091064 seconds, Average Loss: 6.541924357414246\n",
      "Epoch 92 finished in 58.29858207702637 seconds, Average Loss: 6.541079203287761\n",
      "Epoch 93 finished in 56.499571800231934 seconds, Average Loss: 6.541181822617848\n",
      "Epoch 94 finished in 56.526068925857544 seconds, Average Loss: 6.542128562927246\n",
      "Epoch 95 finished in 56.44605112075806 seconds, Average Loss: 6.539373854796092\n",
      "Epoch 96 finished in 56.808858156204224 seconds, Average Loss: 6.539504786332448\n",
      "Epoch 97 finished in 56.460068225860596 seconds, Average Loss: 6.538641373316447\n",
      "Epoch 98 finished in 56.41643786430359 seconds, Average Loss: 6.537747859954834\n",
      "Epoch 99 finished in 56.49018979072571 seconds, Average Loss: 6.536981662114461\n",
      "Epoch 100 finished in 56.37563896179199 seconds, Average Loss: 6.53884361187617\n",
      "Epoch 101 finished in 56.99131917953491 seconds, Average Loss: 6.537967999776204\n",
      "Epoch 102 finished in 57.025190114974976 seconds, Average Loss: 6.538128713766734\n",
      "Epoch 103 finished in 57.26240682601929 seconds, Average Loss: 6.537295440832774\n",
      "Epoch 104 finished in 60.61078190803528 seconds, Average Loss: 6.535600046316783\n",
      "Epoch 105 finished in 57.921083211898804 seconds, Average Loss: 6.536082307497661\n",
      "Epoch 106 finished in 56.41716909408569 seconds, Average Loss: 6.534606854120891\n",
      "Epoch 107 finished in 57.7262909412384 seconds, Average Loss: 6.5353657801946\n",
      "Epoch 108 finished in 57.231231927871704 seconds, Average Loss: 6.535268942515056\n",
      "Epoch 109 finished in 57.15218997001648 seconds, Average Loss: 6.534041126569112\n",
      "Epoch 110 finished in 57.06649303436279 seconds, Average Loss: 6.533603270848592\n",
      "Epoch 111 finished in 57.269405126571655 seconds, Average Loss: 6.5332589745521545\n",
      "Epoch 112 finished in 57.515045166015625 seconds, Average Loss: 6.532525797684987\n",
      "Epoch 113 finished in 56.94669723510742 seconds, Average Loss: 6.534300486246745\n",
      "Epoch 114 finished in 56.39858102798462 seconds, Average Loss: 6.532315870126088\n",
      "Epoch 115 finished in 56.50896215438843 seconds, Average Loss: 6.53246812025706\n",
      "Epoch 116 finished in 56.44350004196167 seconds, Average Loss: 6.533916016419728\n",
      "Epoch 117 finished in 56.71022081375122 seconds, Average Loss: 6.532477577527364\n",
      "Epoch 118 finished in 56.75978231430054 seconds, Average Loss: 6.5318624178568525\n",
      "Epoch 119 finished in 56.36929798126221 seconds, Average Loss: 6.533413489659627\n",
      "Epoch 120 finished in 57.974283933639526 seconds, Average Loss: 6.531054397424062\n",
      "Epoch 121 finished in 57.3836350440979 seconds, Average Loss: 6.531068921089172\n",
      "Epoch 122 finished in 57.05381393432617 seconds, Average Loss: 6.532618006070455\n",
      "Epoch 123 finished in 57.171982765197754 seconds, Average Loss: 6.528803686300914\n",
      "Epoch 124 finished in 56.49792790412903 seconds, Average Loss: 6.530686895052592\n",
      "Epoch 125 finished in 57.855929136276245 seconds, Average Loss: 6.529318888982137\n",
      "Epoch 126 finished in 57.20397400856018 seconds, Average Loss: 6.530322690804799\n",
      "Epoch 127 finished in 56.3147451877594 seconds, Average Loss: 6.52807601292928\n",
      "Epoch 128 finished in 57.04052400588989 seconds, Average Loss: 6.530123670895894\n",
      "Epoch 129 finished in 56.929094076156616 seconds, Average Loss: 6.52880980571111\n",
      "Epoch 130 finished in 56.751993894577026 seconds, Average Loss: 6.528521557648976\n",
      "Epoch 131 finished in 56.445754051208496 seconds, Average Loss: 6.5282964905103045\n",
      "Epoch 132 finished in 56.61288499832153 seconds, Average Loss: 6.528186321258545\n",
      "Epoch 133 finished in 58.31884002685547 seconds, Average Loss: 6.528197964032491\n",
      "Epoch 134 finished in 56.37725210189819 seconds, Average Loss: 6.529366532961528\n",
      "Epoch 135 finished in 57.8065710067749 seconds, Average Loss: 6.526164035002391\n",
      "Epoch 136 finished in 56.86842608451843 seconds, Average Loss: 6.526864171028137\n",
      "Epoch 137 finished in 56.5658438205719 seconds, Average Loss: 6.5265829761823015\n",
      "Epoch 138 finished in 57.6548011302948 seconds, Average Loss: 6.527776638666789\n",
      "Epoch 139 finished in 57.067320108413696 seconds, Average Loss: 6.52663932243983\n",
      "Epoch 140 finished in 57.1001079082489 seconds, Average Loss: 6.5269268949826555\n",
      "Epoch 141 finished in 56.617486000061035 seconds, Average Loss: 6.525980015595754\n",
      "Epoch 142 finished in 56.98986792564392 seconds, Average Loss: 6.52642156680425\n",
      "Epoch 143 finished in 58.369138956069946 seconds, Average Loss: 6.524474263191223\n",
      "Epoch 144 finished in 59.056812047958374 seconds, Average Loss: 6.526312828063965\n",
      "Epoch 145 finished in 57.29694604873657 seconds, Average Loss: 6.525846540927887\n",
      "Epoch 146 finished in 57.17106485366821 seconds, Average Loss: 6.52513066927592\n",
      "Epoch 147 finished in 57.22451996803284 seconds, Average Loss: 6.523564358552297\n",
      "Epoch 148 finished in 58.05532217025757 seconds, Average Loss: 6.524100959300995\n",
      "Epoch 149 finished in 57.87251615524292 seconds, Average Loss: 6.524402459462483\n",
      "Epoch 150 finished in 58.70856499671936 seconds, Average Loss: 6.523747583230336\n",
      "Epoch 151 finished in 57.272687911987305 seconds, Average Loss: 6.522393008073171\n",
      "Epoch 152 finished in 57.25171494483948 seconds, Average Loss: 6.52153476079305\n",
      "Epoch 153 finished in 58.43906927108765 seconds, Average Loss: 6.522822201251984\n",
      "Epoch 154 finished in 58.44838619232178 seconds, Average Loss: 6.522182325522105\n",
      "Epoch 155 finished in 58.4955689907074 seconds, Average Loss: 6.523244579633077\n",
      "Epoch 156 finished in 57.55976319313049 seconds, Average Loss: 6.521952132383983\n",
      "Epoch 157 finished in 59.37094688415527 seconds, Average Loss: 6.5219342311223345\n",
      "Epoch 158 finished in 57.95481610298157 seconds, Average Loss: 6.5215685566266375\n",
      "Epoch 159 finished in 60.048864126205444 seconds, Average Loss: 6.522583266099294\n",
      "Epoch 160 finished in 59.992393016815186 seconds, Average Loss: 6.522627949714661\n",
      "Epoch 161 finished in 58.85479497909546 seconds, Average Loss: 6.521092772483826\n",
      "Epoch 162 finished in 58.51628589630127 seconds, Average Loss: 6.521577099959056\n",
      "Epoch 163 finished in 59.34738898277283 seconds, Average Loss: 6.5216511487960815\n",
      "Epoch 164 finished in 59.88504600524902 seconds, Average Loss: 6.521320879459381\n",
      "Epoch 165 finished in 59.172974824905396 seconds, Average Loss: 6.520417749881744\n",
      "Epoch 166 finished in 60.23021411895752 seconds, Average Loss: 6.521803359190623\n",
      "Epoch 167 finished in 59.65052080154419 seconds, Average Loss: 6.520599524180095\n",
      "Epoch 168 finished in 59.61903786659241 seconds, Average Loss: 6.519864737987518\n",
      "Epoch 169 finished in 59.310593605041504 seconds, Average Loss: 6.520480910936992\n",
      "Epoch 170 finished in 60.03095316886902 seconds, Average Loss: 6.51924200852712\n",
      "Epoch 171 finished in 59.72813534736633 seconds, Average Loss: 6.5221825043360395\n",
      "Epoch 172 finished in 59.50490880012512 seconds, Average Loss: 6.522467990716298\n",
      "Epoch 173 finished in 57.28515315055847 seconds, Average Loss: 6.519796033700307\n",
      "Epoch 174 finished in 57.88926911354065 seconds, Average Loss: 6.520749469598134\n",
      "Epoch 175 finished in 58.6825852394104 seconds, Average Loss: 6.519613703091939\n",
      "Epoch 176 finished in 59.74181914329529 seconds, Average Loss: 6.520312547683716\n",
      "Epoch 177 finished in 59.35917687416077 seconds, Average Loss: 6.520282566547394\n",
      "Epoch 178 finished in 59.11713480949402 seconds, Average Loss: 6.518681267897288\n",
      "Epoch 179 finished in 58.27385497093201 seconds, Average Loss: 6.52129199107488\n",
      "Epoch 180 finished in 58.09978413581848 seconds, Average Loss: 6.519587715466817\n",
      "Epoch 181 finished in 59.58950901031494 seconds, Average Loss: 6.519699374834697\n",
      "Epoch 182 finished in 60.12719202041626 seconds, Average Loss: 6.519538124402364\n",
      "Epoch 183 finished in 59.13258910179138 seconds, Average Loss: 6.520039518674214\n",
      "Epoch 184 finished in 58.54226303100586 seconds, Average Loss: 6.519029478232066\n",
      "Epoch 185 finished in 58.49293494224548 seconds, Average Loss: 6.5187452634175616\n",
      "Epoch 186 finished in 58.8372220993042 seconds, Average Loss: 6.519799828529358\n",
      "Epoch 187 finished in 58.236380100250244 seconds, Average Loss: 6.5187082688013716\n",
      "Epoch 188 finished in 58.82085084915161 seconds, Average Loss: 6.518435895442963\n",
      "Epoch 189 finished in 57.93976879119873 seconds, Average Loss: 6.519335091114044\n",
      "Epoch 190 finished in 58.73047685623169 seconds, Average Loss: 6.519483546415965\n",
      "Epoch 191 finished in 58.758902072906494 seconds, Average Loss: 6.519129852453868\n",
      "Epoch 192 finished in 58.01303720474243 seconds, Average Loss: 6.5194964210192365\n",
      "Epoch 193 finished in 56.888580083847046 seconds, Average Loss: 6.51908540725708\n",
      "Epoch 194 finished in 57.613094091415405 seconds, Average Loss: 6.519669870535533\n",
      "Epoch 195 finished in 57.16214609146118 seconds, Average Loss: 6.51829473177592\n",
      "Epoch 196 finished in 56.342121839523315 seconds, Average Loss: 6.5167390902837115\n",
      "Epoch 197 finished in 59.15751910209656 seconds, Average Loss: 6.519409120082855\n",
      "Epoch 198 finished in 59.66175198554993 seconds, Average Loss: 6.519739846388499\n",
      "Epoch 199 finished in 58.55301117897034 seconds, Average Loss: 6.518217285474141\n",
      "Epoch 200 finished in 58.77575087547302 seconds, Average Loss: 6.5191420912742615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f580b0f0c1be40588b67701a54c8cab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.51914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-deluge-11</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/pakm67xc' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/pakm67xc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240325_234831-pakm67xc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240326_030057-60y1vpf1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/60y1vpf1' target=\"_blank\">amber-dawn-12</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/60y1vpf1' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/60y1vpf1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 64.12334609031677 seconds, Average Loss: 7.8809581597646075\n",
      "Epoch 2 finished in 63.78705406188965 seconds, Average Loss: 7.512868881225586\n",
      "Epoch 3 finished in 63.828185081481934 seconds, Average Loss: 7.361448387304942\n",
      "Epoch 4 finished in 63.617778062820435 seconds, Average Loss: 7.284385820229848\n",
      "Epoch 5 finished in 64.01339626312256 seconds, Average Loss: 7.23276025056839\n",
      "Epoch 6 finished in 63.590866804122925 seconds, Average Loss: 7.1968793869018555\n",
      "Epoch 7 finished in 63.967740297317505 seconds, Average Loss: 7.165963311990102\n",
      "Epoch 8 finished in 64.11494207382202 seconds, Average Loss: 7.140089909235637\n",
      "Epoch 9 finished in 64.20772409439087 seconds, Average Loss: 7.1170707543691\n",
      "Epoch 10 finished in 63.92160487174988 seconds, Average Loss: 7.095264752705892\n",
      "Epoch 11 finished in 64.0262598991394 seconds, Average Loss: 7.072635571161906\n",
      "Epoch 12 finished in 64.26510190963745 seconds, Average Loss: 7.057318568229675\n",
      "Epoch 13 finished in 64.65180015563965 seconds, Average Loss: 7.034851153691609\n",
      "Epoch 14 finished in 64.35815095901489 seconds, Average Loss: 7.018089771270752\n",
      "Epoch 15 finished in 64.13429999351501 seconds, Average Loss: 7.002398471037547\n",
      "Epoch 16 finished in 64.44778800010681 seconds, Average Loss: 6.982376078764598\n",
      "Epoch 17 finished in 64.33274388313293 seconds, Average Loss: 6.978218654791514\n",
      "Epoch 18 finished in 64.23569512367249 seconds, Average Loss: 6.964947283267975\n",
      "Epoch 19 finished in 63.926790952682495 seconds, Average Loss: 6.954052766164144\n",
      "Epoch 20 finished in 64.12762689590454 seconds, Average Loss: 6.944518466790517\n",
      "Epoch 21 finished in 64.28675723075867 seconds, Average Loss: 6.938205540180206\n",
      "Epoch 22 finished in 64.14339590072632 seconds, Average Loss: 6.927193224430084\n",
      "Epoch 23 finished in 64.1989860534668 seconds, Average Loss: 6.9206132888793945\n",
      "Epoch 24 finished in 64.19637417793274 seconds, Average Loss: 6.912153939406077\n",
      "Epoch 25 finished in 64.17622494697571 seconds, Average Loss: 6.9078207810719805\n",
      "Epoch 26 finished in 64.30913591384888 seconds, Average Loss: 6.903693497180939\n",
      "Epoch 27 finished in 64.34883713722229 seconds, Average Loss: 6.897087454795837\n",
      "Epoch 28 finished in 64.4202811717987 seconds, Average Loss: 6.8889709909756975\n",
      "Epoch 29 finished in 64.1398777961731 seconds, Average Loss: 6.883753259976705\n",
      "Epoch 30 finished in 64.3568127155304 seconds, Average Loss: 6.8793551325798035\n",
      "Epoch 31 finished in 64.18149709701538 seconds, Average Loss: 6.8753727078437805\n",
      "Epoch 32 finished in 64.40082788467407 seconds, Average Loss: 6.871140003204346\n",
      "Epoch 33 finished in 64.15214991569519 seconds, Average Loss: 6.8645732800165815\n",
      "Epoch 34 finished in 63.84935402870178 seconds, Average Loss: 6.857242067654927\n",
      "Epoch 35 finished in 64.01512408256531 seconds, Average Loss: 6.8583663900693255\n",
      "Epoch 36 finished in 64.01040601730347 seconds, Average Loss: 6.851901829242706\n",
      "Epoch 37 finished in 64.3781681060791 seconds, Average Loss: 6.851085106531779\n",
      "Epoch 38 finished in 64.00568294525146 seconds, Average Loss: 6.8462822039922075\n",
      "Epoch 39 finished in 64.02512907981873 seconds, Average Loss: 6.840637187163035\n",
      "Epoch 40 finished in 64.69065403938293 seconds, Average Loss: 6.841375648975372\n",
      "Epoch 41 finished in 64.26911807060242 seconds, Average Loss: 6.836850722630818\n",
      "Epoch 42 finished in 64.22470188140869 seconds, Average Loss: 6.830779314041138\n",
      "Epoch 43 finished in 64.02788519859314 seconds, Average Loss: 6.832833866278331\n",
      "Epoch 44 finished in 63.9086389541626 seconds, Average Loss: 6.825370331605275\n",
      "Epoch 45 finished in 63.943297147750854 seconds, Average Loss: 6.82350879907608\n",
      "Epoch 46 finished in 64.01696491241455 seconds, Average Loss: 6.820955733458201\n",
      "Epoch 47 finished in 63.94224691390991 seconds, Average Loss: 6.818057894706726\n",
      "Epoch 48 finished in 64.12768316268921 seconds, Average Loss: 6.816835741202037\n",
      "Epoch 49 finished in 64.07940888404846 seconds, Average Loss: 6.817386070887248\n",
      "Epoch 50 finished in 64.04058599472046 seconds, Average Loss: 6.811073740323384\n",
      "Epoch 51 finished in 64.21538877487183 seconds, Average Loss: 6.810654123624166\n",
      "Epoch 52 finished in 64.00590705871582 seconds, Average Loss: 6.80826997756958\n",
      "Epoch 53 finished in 64.3424608707428 seconds, Average Loss: 6.806848804155986\n",
      "Epoch 54 finished in 63.957571029663086 seconds, Average Loss: 6.80450560649236\n",
      "Epoch 55 finished in 63.86955213546753 seconds, Average Loss: 6.8014687697092695\n",
      "Epoch 56 finished in 64.39987778663635 seconds, Average Loss: 6.798074563344319\n",
      "Epoch 57 finished in 64.15965604782104 seconds, Average Loss: 6.7973513007164\n",
      "Epoch 58 finished in 64.17851901054382 seconds, Average Loss: 6.794317603111267\n",
      "Epoch 59 finished in 64.13626313209534 seconds, Average Loss: 6.788463930288951\n",
      "Epoch 60 finished in 64.16478228569031 seconds, Average Loss: 6.789121548334758\n",
      "Epoch 61 finished in 64.00057291984558 seconds, Average Loss: 6.786242822806041\n",
      "Epoch 62 finished in 64.22358989715576 seconds, Average Loss: 6.785757442315419\n",
      "Epoch 63 finished in 63.89868116378784 seconds, Average Loss: 6.78964885075887\n",
      "Epoch 64 finished in 63.9383909702301 seconds, Average Loss: 6.784771621227264\n",
      "Epoch 65 finished in 64.12438702583313 seconds, Average Loss: 6.782094856103261\n",
      "Epoch 66 finished in 64.42145919799805 seconds, Average Loss: 6.781537552674611\n",
      "Epoch 67 finished in 64.0101068019867 seconds, Average Loss: 6.779032588005066\n",
      "Epoch 68 finished in 64.31915426254272 seconds, Average Loss: 6.779571692148845\n",
      "Epoch 69 finished in 64.36794590950012 seconds, Average Loss: 6.778721074263255\n",
      "Epoch 70 finished in 64.31422591209412 seconds, Average Loss: 6.774541060129802\n",
      "Epoch 71 finished in 64.4968318939209 seconds, Average Loss: 6.774119416872661\n",
      "Epoch 72 finished in 64.74682402610779 seconds, Average Loss: 6.775566458702087\n",
      "Epoch 73 finished in 64.06147623062134 seconds, Average Loss: 6.773391266663869\n",
      "Epoch 74 finished in 64.30421304702759 seconds, Average Loss: 6.770111163457234\n",
      "Epoch 75 finished in 64.35907793045044 seconds, Average Loss: 6.769497374693553\n",
      "Epoch 76 finished in 64.24627494812012 seconds, Average Loss: 6.7710767189661665\n",
      "Epoch 77 finished in 64.134516954422 seconds, Average Loss: 6.767179290453593\n",
      "Epoch 78 finished in 64.21699523925781 seconds, Average Loss: 6.7658465305964155\n",
      "Epoch 79 finished in 64.48386716842651 seconds, Average Loss: 6.761007924874623\n",
      "Epoch 80 finished in 64.04682111740112 seconds, Average Loss: 6.76316112279892\n",
      "Epoch 81 finished in 65.5172791481018 seconds, Average Loss: 6.760222673416138\n",
      "Epoch 82 finished in 64.16099619865417 seconds, Average Loss: 6.760028600692749\n",
      "Epoch 83 finished in 64.01158714294434 seconds, Average Loss: 6.7609679500261946\n",
      "Epoch 84 finished in 64.20513677597046 seconds, Average Loss: 6.761384725570679\n",
      "Epoch 85 finished in 63.991109132766724 seconds, Average Loss: 6.75626661380132\n",
      "Epoch 86 finished in 63.838635206222534 seconds, Average Loss: 6.755069772402446\n",
      "Epoch 87 finished in 63.81340503692627 seconds, Average Loss: 6.755018432935079\n",
      "Epoch 88 finished in 64.04039764404297 seconds, Average Loss: 6.754840075969696\n",
      "Epoch 89 finished in 63.91109108924866 seconds, Average Loss: 6.752284109592438\n",
      "Epoch 90 finished in 63.770915031433105 seconds, Average Loss: 6.755274156729381\n",
      "Epoch 91 finished in 63.952369928359985 seconds, Average Loss: 6.7535525759061175\n",
      "Epoch 92 finished in 63.720520973205566 seconds, Average Loss: 6.749237517515819\n",
      "Epoch 93 finished in 63.900222063064575 seconds, Average Loss: 6.751206775506337\n",
      "Epoch 94 finished in 64.04736590385437 seconds, Average Loss: 6.747483730316162\n",
      "Epoch 95 finished in 63.79260206222534 seconds, Average Loss: 6.746688822905223\n",
      "Epoch 96 finished in 64.46405506134033 seconds, Average Loss: 6.741869807243347\n",
      "Epoch 97 finished in 64.83607316017151 seconds, Average Loss: 6.7474245429039\n",
      "Epoch 98 finished in 63.78415775299072 seconds, Average Loss: 6.7479686339696245\n",
      "Epoch 99 finished in 63.81981372833252 seconds, Average Loss: 6.74435422817866\n",
      "Epoch 100 finished in 63.65796780586243 seconds, Average Loss: 6.740483283996582\n",
      "Epoch 101 finished in 63.738542795181274 seconds, Average Loss: 6.742034514745076\n",
      "Epoch 102 finished in 63.69496178627014 seconds, Average Loss: 6.740835964679718\n",
      "Epoch 103 finished in 63.72419190406799 seconds, Average Loss: 6.737087349096934\n",
      "Epoch 104 finished in 63.85883283615112 seconds, Average Loss: 6.738866806030273\n",
      "Epoch 105 finished in 64.05185198783875 seconds, Average Loss: 6.738506178061168\n",
      "Epoch 106 finished in 63.79858207702637 seconds, Average Loss: 6.733966847260793\n",
      "Epoch 107 finished in 64.03059935569763 seconds, Average Loss: 6.7377698222796125\n",
      "Epoch 108 finished in 63.76942420005798 seconds, Average Loss: 6.73623389005661\n",
      "Epoch 109 finished in 63.8262140750885 seconds, Average Loss: 6.737945040067037\n",
      "Epoch 110 finished in 64.00929307937622 seconds, Average Loss: 6.7351381580034895\n",
      "Epoch 111 finished in 63.66812586784363 seconds, Average Loss: 6.732681393623352\n",
      "Epoch 112 finished in 64.78106379508972 seconds, Average Loss: 6.732364972432454\n",
      "Epoch 113 finished in 64.26361989974976 seconds, Average Loss: 6.731884777545929\n",
      "Epoch 114 finished in 63.95985007286072 seconds, Average Loss: 6.729349573453267\n",
      "Epoch 115 finished in 64.02656888961792 seconds, Average Loss: 6.730985502401988\n",
      "Epoch 116 finished in 64.17175078392029 seconds, Average Loss: 6.732307056585948\n",
      "Epoch 117 finished in 64.27050185203552 seconds, Average Loss: 6.729732890923818\n",
      "Epoch 118 finished in 64.05696511268616 seconds, Average Loss: 6.730326573053996\n",
      "Epoch 119 finished in 64.14781999588013 seconds, Average Loss: 6.727372845013936\n",
      "Epoch 120 finished in 64.14522886276245 seconds, Average Loss: 6.726734101772308\n",
      "Epoch 121 finished in 64.42808079719543 seconds, Average Loss: 6.727570474147797\n",
      "Epoch 122 finished in 64.04451513290405 seconds, Average Loss: 6.726688961187999\n",
      "Epoch 123 finished in 64.15616106987 seconds, Average Loss: 6.720850030581157\n",
      "Epoch 124 finished in 64.07463002204895 seconds, Average Loss: 6.725965301195781\n",
      "Epoch 125 finished in 64.04897284507751 seconds, Average Loss: 6.722273786862691\n",
      "Epoch 126 finished in 64.79534316062927 seconds, Average Loss: 6.723272542158763\n",
      "Epoch 127 finished in 64.26948690414429 seconds, Average Loss: 6.722697377204895\n",
      "Epoch 128 finished in 64.46235394477844 seconds, Average Loss: 6.721923967202504\n",
      "Epoch 129 finished in 64.50474619865417 seconds, Average Loss: 6.722469429175059\n",
      "Epoch 130 finished in 65.2801878452301 seconds, Average Loss: 6.722008804480235\n",
      "Epoch 131 finished in 64.21411514282227 seconds, Average Loss: 6.718323210875194\n",
      "Epoch 132 finished in 63.994179010391235 seconds, Average Loss: 6.719413340091705\n",
      "Epoch 133 finished in 64.30120873451233 seconds, Average Loss: 6.7200601895650225\n",
      "Epoch 134 finished in 63.93760395050049 seconds, Average Loss: 6.719325661659241\n",
      "Epoch 135 finished in 64.09692692756653 seconds, Average Loss: 6.721693178017934\n",
      "Epoch 136 finished in 64.16191101074219 seconds, Average Loss: 6.714161495367686\n",
      "Epoch 137 finished in 63.8688280582428 seconds, Average Loss: 6.716913302739461\n",
      "Epoch 138 finished in 63.93762397766113 seconds, Average Loss: 6.716320256392161\n",
      "Epoch 139 finished in 64.11889791488647 seconds, Average Loss: 6.716273188591003\n",
      "Epoch 140 finished in 64.04982781410217 seconds, Average Loss: 6.71705440680186\n",
      "Epoch 141 finished in 63.770853996276855 seconds, Average Loss: 6.7154314716657\n",
      "Epoch 142 finished in 64.06294798851013 seconds, Average Loss: 6.711848179499309\n",
      "Epoch 143 finished in 63.90676474571228 seconds, Average Loss: 6.7135010957717896\n",
      "Epoch 144 finished in 63.966124057769775 seconds, Average Loss: 6.713963866233826\n",
      "Epoch 145 finished in 64.33343887329102 seconds, Average Loss: 6.714395642280579\n",
      "Epoch 146 finished in 63.90502190589905 seconds, Average Loss: 6.711207052071889\n",
      "Epoch 147 finished in 63.96447205543518 seconds, Average Loss: 6.709667603174846\n",
      "Epoch 148 finished in 63.94845795631409 seconds, Average Loss: 6.711783627669017\n",
      "Epoch 149 finished in 64.10208511352539 seconds, Average Loss: 6.710953652858734\n",
      "Epoch 150 finished in 63.93597960472107 seconds, Average Loss: 6.71114448706309\n",
      "Epoch 151 finished in 64.16552209854126 seconds, Average Loss: 6.710293968518575\n",
      "Epoch 152 finished in 64.13046503067017 seconds, Average Loss: 6.709291477998097\n",
      "Epoch 153 finished in 64.18198585510254 seconds, Average Loss: 6.710954487323761\n",
      "Epoch 154 finished in 64.42793607711792 seconds, Average Loss: 6.709011673927307\n",
      "Epoch 155 finished in 64.06555008888245 seconds, Average Loss: 6.706014772256215\n",
      "Epoch 156 finished in 63.952789068222046 seconds, Average Loss: 6.709512770175934\n",
      "Epoch 157 finished in 63.89274597167969 seconds, Average Loss: 6.710257053375244\n",
      "Epoch 158 finished in 63.921849966049194 seconds, Average Loss: 6.707669715086619\n",
      "Epoch 159 finished in 63.883888959884644 seconds, Average Loss: 6.706366856892903\n",
      "Epoch 160 finished in 63.92786192893982 seconds, Average Loss: 6.707665940125783\n",
      "Epoch 161 finished in 64.34071016311646 seconds, Average Loss: 6.706654866536458\n",
      "Epoch 162 finished in 64.03893399238586 seconds, Average Loss: 6.705931683381398\n",
      "Epoch 163 finished in 63.95957279205322 seconds, Average Loss: 6.702672819296519\n",
      "Epoch 164 finished in 64.09641861915588 seconds, Average Loss: 6.705116629600525\n",
      "Epoch 165 finished in 64.22521185874939 seconds, Average Loss: 6.704636613527934\n",
      "Epoch 166 finished in 64.03858304023743 seconds, Average Loss: 6.702726384003957\n",
      "Epoch 167 finished in 63.95987296104431 seconds, Average Loss: 6.703774015108745\n",
      "Epoch 168 finished in 64.73965406417847 seconds, Average Loss: 6.704181810220082\n",
      "Epoch 169 finished in 63.866700649261475 seconds, Average Loss: 6.7009790142377215\n",
      "Epoch 170 finished in 63.848893880844116 seconds, Average Loss: 6.706659495830536\n",
      "Epoch 171 finished in 63.867676973342896 seconds, Average Loss: 6.70456592241923\n",
      "Epoch 172 finished in 64.03627490997314 seconds, Average Loss: 6.702543993790944\n",
      "Epoch 173 finished in 64.11221385002136 seconds, Average Loss: 6.70300163825353\n",
      "Epoch 174 finished in 63.90130400657654 seconds, Average Loss: 6.70181006193161\n",
      "Epoch 175 finished in 63.91010904312134 seconds, Average Loss: 6.7043459216753645\n",
      "Epoch 176 finished in 63.98663878440857 seconds, Average Loss: 6.702413181463878\n",
      "Epoch 177 finished in 64.32688784599304 seconds, Average Loss: 6.701920370260875\n",
      "Epoch 178 finished in 64.01101684570312 seconds, Average Loss: 6.701375583807628\n",
      "Epoch 179 finished in 64.07876491546631 seconds, Average Loss: 6.70029354095459\n",
      "Epoch 180 finished in 64.09536695480347 seconds, Average Loss: 6.700621485710144\n",
      "Epoch 181 finished in 64.14364385604858 seconds, Average Loss: 6.704116026560466\n",
      "Epoch 182 finished in 64.54035019874573 seconds, Average Loss: 6.701755324999492\n",
      "Epoch 183 finished in 64.21817898750305 seconds, Average Loss: 6.699692706267039\n",
      "Epoch 184 finished in 64.43587303161621 seconds, Average Loss: 6.703074316183726\n",
      "Epoch 185 finished in 64.25484204292297 seconds, Average Loss: 6.702425201733907\n",
      "Epoch 186 finished in 64.16589689254761 seconds, Average Loss: 6.700350721677144\n",
      "Epoch 187 finished in 64.3078339099884 seconds, Average Loss: 6.699370245138804\n",
      "Epoch 188 finished in 64.05231428146362 seconds, Average Loss: 6.7013291120529175\n",
      "Epoch 189 finished in 64.06074857711792 seconds, Average Loss: 6.701521257559459\n",
      "Epoch 190 finished in 64.28433394432068 seconds, Average Loss: 6.7001491983731585\n",
      "Epoch 191 finished in 64.2207670211792 seconds, Average Loss: 6.697962423165639\n",
      "Epoch 192 finished in 64.57571363449097 seconds, Average Loss: 6.702059288819631\n",
      "Epoch 193 finished in 64.34814500808716 seconds, Average Loss: 6.701964179674785\n",
      "Epoch 194 finished in 63.91480898857117 seconds, Average Loss: 6.699567536513011\n",
      "Epoch 195 finished in 64.30134677886963 seconds, Average Loss: 6.701304177443187\n",
      "Epoch 196 finished in 64.31237697601318 seconds, Average Loss: 6.701706409454346\n",
      "Epoch 197 finished in 64.4810209274292 seconds, Average Loss: 6.701746900876363\n",
      "Epoch 198 finished in 64.34547996520996 seconds, Average Loss: 6.70098461707433\n",
      "Epoch 199 finished in 64.02265095710754 seconds, Average Loss: 6.699271202087402\n",
      "Epoch 200 finished in 64.34159302711487 seconds, Average Loss: 6.700908303260803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca35109b69ff4084975368f5cc6e56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.70091</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-dawn-12</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/60y1vpf1' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/60y1vpf1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_030057-60y1vpf1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240326_063452-yxic3ujx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/yxic3ujx' target=\"_blank\">fluent-glitter-13</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/yxic3ujx' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/yxic3ujx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 79.09375405311584 seconds, Average Loss: 7.618877291679382\n",
      "Epoch 2 finished in 76.70164394378662 seconds, Average Loss: 7.126332461833954\n",
      "Epoch 3 finished in 78.1787121295929 seconds, Average Loss: 6.969899157683055\n",
      "Epoch 4 finished in 77.82795906066895 seconds, Average Loss: 6.879142880439758\n",
      "Epoch 5 finished in 79.54167127609253 seconds, Average Loss: 6.830473442872365\n",
      "Epoch 6 finished in 79.62723207473755 seconds, Average Loss: 6.790880044301351\n",
      "Epoch 7 finished in 79.19032907485962 seconds, Average Loss: 6.764280537764232\n",
      "Epoch 8 finished in 78.93450808525085 seconds, Average Loss: 6.7430077989896136\n",
      "Epoch 9 finished in 79.53180432319641 seconds, Average Loss: 6.729019582271576\n",
      "Epoch 10 finished in 79.00721406936646 seconds, Average Loss: 6.71087114016215\n",
      "Epoch 11 finished in 78.33979511260986 seconds, Average Loss: 6.696597874164581\n",
      "Epoch 12 finished in 77.17670392990112 seconds, Average Loss: 6.688126703103383\n",
      "Epoch 13 finished in 78.64353203773499 seconds, Average Loss: 6.675410330295563\n",
      "Epoch 14 finished in 76.98385190963745 seconds, Average Loss: 6.668610215187073\n",
      "Epoch 15 finished in 78.42957091331482 seconds, Average Loss: 6.657443126042684\n",
      "Epoch 16 finished in 78.34977412223816 seconds, Average Loss: 6.650916715463002\n",
      "Epoch 17 finished in 76.85960912704468 seconds, Average Loss: 6.645817120869954\n",
      "Epoch 18 finished in 78.91349291801453 seconds, Average Loss: 6.639771521091461\n",
      "Epoch 19 finished in 78.65825486183167 seconds, Average Loss: 6.633101900418599\n",
      "Epoch 20 finished in 80.08710289001465 seconds, Average Loss: 6.627650201320648\n",
      "Epoch 21 finished in 80.3023931980133 seconds, Average Loss: 6.626652697722117\n",
      "Epoch 22 finished in 79.11968517303467 seconds, Average Loss: 6.620884716510773\n",
      "Epoch 23 finished in 79.66811299324036 seconds, Average Loss: 6.617853860060374\n",
      "Epoch 24 finished in 79.94127178192139 seconds, Average Loss: 6.615320761998494\n",
      "Epoch 25 finished in 79.17989826202393 seconds, Average Loss: 6.613114734490712\n",
      "Epoch 26 finished in 80.02327108383179 seconds, Average Loss: 6.609202702840169\n",
      "Epoch 27 finished in 80.07230305671692 seconds, Average Loss: 6.608030279477437\n",
      "Epoch 28 finished in 79.46067190170288 seconds, Average Loss: 6.603786885738373\n",
      "Epoch 29 finished in 80.30391597747803 seconds, Average Loss: 6.600054959456126\n",
      "Epoch 30 finished in 80.0375018119812 seconds, Average Loss: 6.599645773569743\n",
      "Epoch 31 finished in 80.68646883964539 seconds, Average Loss: 6.598936498165131\n",
      "Epoch 32 finished in 79.7415292263031 seconds, Average Loss: 6.597403407096863\n",
      "Epoch 33 finished in 80.74342393875122 seconds, Average Loss: 6.59571514527003\n",
      "Epoch 34 finished in 79.49207711219788 seconds, Average Loss: 6.59288905064265\n",
      "Epoch 35 finished in 79.44707822799683 seconds, Average Loss: 6.590816020965576\n",
      "Epoch 36 finished in 79.19959878921509 seconds, Average Loss: 6.590728481610616\n",
      "Epoch 37 finished in 79.068843126297 seconds, Average Loss: 6.587874035040538\n",
      "Epoch 38 finished in 77.27730989456177 seconds, Average Loss: 6.586897532145183\n",
      "Epoch 39 finished in 79.32411694526672 seconds, Average Loss: 6.584724148114522\n",
      "Epoch 40 finished in 79.28913402557373 seconds, Average Loss: 6.585578083992004\n",
      "Epoch 41 finished in 78.99717998504639 seconds, Average Loss: 6.583079318205516\n",
      "Epoch 42 finished in 79.0726170539856 seconds, Average Loss: 6.581157684326172\n",
      "Epoch 43 finished in 78.66359305381775 seconds, Average Loss: 6.581530570983887\n",
      "Epoch 44 finished in 78.37464189529419 seconds, Average Loss: 6.579064826170604\n",
      "Epoch 45 finished in 77.01436376571655 seconds, Average Loss: 6.579843084017436\n",
      "Epoch 46 finished in 79.6479389667511 seconds, Average Loss: 6.576795697212219\n",
      "Epoch 47 finished in 80.16462206840515 seconds, Average Loss: 6.576721588770549\n",
      "Epoch 48 finished in 80.20556473731995 seconds, Average Loss: 6.575242618719737\n",
      "Epoch 49 finished in 79.79275298118591 seconds, Average Loss: 6.57607380549113\n",
      "Epoch 50 finished in 79.70140790939331 seconds, Average Loss: 6.57209046681722\n",
      "Epoch 51 finished in 79.83559799194336 seconds, Average Loss: 6.57168443997701\n",
      "Epoch 52 finished in 79.79881000518799 seconds, Average Loss: 6.572412212689717\n",
      "Epoch 53 finished in 79.12868809700012 seconds, Average Loss: 6.569828848044078\n",
      "Epoch 54 finished in 79.2333869934082 seconds, Average Loss: 6.5693255464235945\n",
      "Epoch 55 finished in 80.00104188919067 seconds, Average Loss: 6.567479948202769\n",
      "Epoch 56 finished in 79.85019278526306 seconds, Average Loss: 6.567781507968903\n",
      "Epoch 57 finished in 78.34289264678955 seconds, Average Loss: 6.56779803832372\n",
      "Epoch 58 finished in 79.32921814918518 seconds, Average Loss: 6.56534477074941\n",
      "Epoch 59 finished in 77.51442694664001 seconds, Average Loss: 6.563282489776611\n",
      "Epoch 60 finished in 79.208340883255 seconds, Average Loss: 6.565638442834218\n",
      "Epoch 61 finished in 76.48126316070557 seconds, Average Loss: 6.563551247119904\n",
      "Epoch 62 finished in 76.591481924057 seconds, Average Loss: 6.562221169471741\n",
      "Epoch 63 finished in 77.52244877815247 seconds, Average Loss: 6.561140398184459\n",
      "Epoch 64 finished in 78.1506838798523 seconds, Average Loss: 6.560805559158325\n",
      "Epoch 65 finished in 78.73208284378052 seconds, Average Loss: 6.561092793941498\n",
      "Epoch 66 finished in 78.66600584983826 seconds, Average Loss: 6.561014473438263\n",
      "Epoch 67 finished in 79.24117708206177 seconds, Average Loss: 6.560855507850647\n",
      "Epoch 68 finished in 78.65678286552429 seconds, Average Loss: 6.560219426949819\n",
      "Epoch 69 finished in 78.22050476074219 seconds, Average Loss: 6.55891078710556\n",
      "Epoch 70 finished in 77.04423308372498 seconds, Average Loss: 6.55784797668457\n",
      "Epoch 71 finished in 77.7930371761322 seconds, Average Loss: 6.557984312375386\n",
      "Epoch 72 finished in 78.75483989715576 seconds, Average Loss: 6.557111481825511\n",
      "Epoch 73 finished in 79.96456003189087 seconds, Average Loss: 6.557128270467122\n",
      "Epoch 74 finished in 79.09300780296326 seconds, Average Loss: 6.555509388446808\n",
      "Epoch 75 finished in 77.15263104438782 seconds, Average Loss: 6.554773112138112\n",
      "Epoch 76 finished in 78.99886631965637 seconds, Average Loss: 6.555105447769165\n",
      "Epoch 77 finished in 79.63648390769958 seconds, Average Loss: 6.555081228415172\n",
      "Epoch 78 finished in 79.61554408073425 seconds, Average Loss: 6.554652551809947\n",
      "Epoch 79 finished in 79.69091010093689 seconds, Average Loss: 6.55184531211853\n",
      "Epoch 80 finished in 78.87601208686829 seconds, Average Loss: 6.55297193924586\n",
      "Epoch 81 finished in 79.76711797714233 seconds, Average Loss: 6.551141063372294\n",
      "Epoch 82 finished in 79.21218800544739 seconds, Average Loss: 6.552462736765544\n",
      "Epoch 83 finished in 80.12889409065247 seconds, Average Loss: 6.549639602502187\n",
      "Epoch 84 finished in 80.35205101966858 seconds, Average Loss: 6.55091667175293\n",
      "Epoch 85 finished in 79.22534608840942 seconds, Average Loss: 6.550021270910899\n",
      "Epoch 86 finished in 78.94782304763794 seconds, Average Loss: 6.5499656200408936\n",
      "Epoch 87 finished in 79.73080086708069 seconds, Average Loss: 6.550859113534291\n",
      "Epoch 88 finished in 80.96051716804504 seconds, Average Loss: 6.548769215742747\n",
      "Epoch 89 finished in 83.91023898124695 seconds, Average Loss: 6.550324817498525\n",
      "Epoch 90 finished in 81.03954792022705 seconds, Average Loss: 6.5498546957969666\n",
      "Epoch 91 finished in 82.02401494979858 seconds, Average Loss: 6.548763235410054\n",
      "Epoch 92 finished in 79.71694898605347 seconds, Average Loss: 6.550122141838074\n",
      "Epoch 93 finished in 85.26608610153198 seconds, Average Loss: 6.548794170220693\n",
      "Epoch 94 finished in 78.8035798072815 seconds, Average Loss: 6.546533584594727\n",
      "Epoch 95 finished in 77.11610388755798 seconds, Average Loss: 6.5470549662907915\n",
      "Epoch 96 finished in 77.70696187019348 seconds, Average Loss: 6.544214228789012\n",
      "Epoch 97 finished in 77.4717378616333 seconds, Average Loss: 6.544596493244171\n",
      "Epoch 98 finished in 79.8709979057312 seconds, Average Loss: 6.545648177464803\n",
      "Epoch 99 finished in 76.77256488800049 seconds, Average Loss: 6.543346107006073\n",
      "Epoch 100 finished in 76.44769716262817 seconds, Average Loss: 6.541852494080861\n",
      "Epoch 101 finished in 79.94417190551758 seconds, Average Loss: 6.543277025222778\n",
      "Epoch 102 finished in 79.1873619556427 seconds, Average Loss: 6.54274966319402\n",
      "Epoch 103 finished in 76.56562614440918 seconds, Average Loss: 6.543123523394267\n",
      "Epoch 104 finished in 76.73594808578491 seconds, Average Loss: 6.541890025138855\n",
      "Epoch 105 finished in 77.31515789031982 seconds, Average Loss: 6.540803511937459\n",
      "Epoch 106 finished in 76.50227522850037 seconds, Average Loss: 6.541435698668162\n",
      "Epoch 107 finished in 77.43534111976624 seconds, Average Loss: 6.5413998166720075\n",
      "Epoch 108 finished in 78.2288019657135 seconds, Average Loss: 6.541397869586945\n",
      "Epoch 109 finished in 76.57725977897644 seconds, Average Loss: 6.54166845480601\n",
      "Epoch 110 finished in 77.19585514068604 seconds, Average Loss: 6.53964348634084\n",
      "Epoch 111 finished in 79.7141740322113 seconds, Average Loss: 6.540843486785889\n",
      "Epoch 112 finished in 77.31045413017273 seconds, Average Loss: 6.540596167246501\n",
      "Epoch 113 finished in 77.87970995903015 seconds, Average Loss: 6.54103285074234\n",
      "Epoch 114 finished in 77.27641201019287 seconds, Average Loss: 6.538109719753265\n",
      "Epoch 115 finished in 79.31144380569458 seconds, Average Loss: 6.538350264231364\n",
      "Epoch 116 finished in 78.63946485519409 seconds, Average Loss: 6.537000358104706\n",
      "Epoch 117 finished in 77.56400179862976 seconds, Average Loss: 6.538151999314626\n",
      "Epoch 118 finished in 79.06305384635925 seconds, Average Loss: 6.535671571890513\n",
      "Epoch 119 finished in 79.45231580734253 seconds, Average Loss: 6.536655863126119\n",
      "Epoch 120 finished in 77.83397698402405 seconds, Average Loss: 6.535957455635071\n",
      "Epoch 121 finished in 79.29404807090759 seconds, Average Loss: 6.538423438866933\n",
      "Epoch 122 finished in 79.63284301757812 seconds, Average Loss: 6.535833775997162\n",
      "Epoch 123 finished in 78.472571849823 seconds, Average Loss: 6.5353156725565595\n",
      "Epoch 124 finished in 76.5507562160492 seconds, Average Loss: 6.535751501719157\n",
      "Epoch 125 finished in 78.90377616882324 seconds, Average Loss: 6.534818251927693\n",
      "Epoch 126 finished in 77.33703708648682 seconds, Average Loss: 6.5368448694547014\n",
      "Epoch 127 finished in 78.94494676589966 seconds, Average Loss: 6.536461631457011\n",
      "Epoch 128 finished in 76.67900109291077 seconds, Average Loss: 6.534104029337565\n",
      "Epoch 129 finished in 78.79056787490845 seconds, Average Loss: 6.535180370012919\n",
      "Epoch 130 finished in 76.8504707813263 seconds, Average Loss: 6.534578641255696\n",
      "Epoch 131 finished in 76.97638177871704 seconds, Average Loss: 6.5335585077603655\n",
      "Epoch 132 finished in 78.68704795837402 seconds, Average Loss: 6.533807655175527\n",
      "Epoch 133 finished in 77.29460573196411 seconds, Average Loss: 6.5341847737630205\n",
      "Epoch 134 finished in 76.9109559059143 seconds, Average Loss: 6.532223125298818\n",
      "Epoch 135 finished in 78.51797199249268 seconds, Average Loss: 6.533048848311107\n",
      "Epoch 136 finished in 76.61433219909668 seconds, Average Loss: 6.53173170487086\n",
      "Epoch 137 finished in 76.65017104148865 seconds, Average Loss: 6.531794826189677\n",
      "Epoch 138 finished in 79.22779417037964 seconds, Average Loss: 6.533038179079692\n",
      "Epoch 139 finished in 76.72988200187683 seconds, Average Loss: 6.532094160715739\n",
      "Epoch 140 finished in 79.82215309143066 seconds, Average Loss: 6.532223880290985\n",
      "Epoch 141 finished in 80.0476143360138 seconds, Average Loss: 6.531306028366089\n",
      "Epoch 142 finished in 78.01961970329285 seconds, Average Loss: 6.53051221370697\n",
      "Epoch 143 finished in 78.18511009216309 seconds, Average Loss: 6.530967017014821\n",
      "Epoch 144 finished in 79.62257504463196 seconds, Average Loss: 6.532609303792317\n",
      "Epoch 145 finished in 77.65751504898071 seconds, Average Loss: 6.529971758524577\n",
      "Epoch 146 finished in 77.21838283538818 seconds, Average Loss: 6.5289709369341535\n",
      "Epoch 147 finished in 77.14252305030823 seconds, Average Loss: 6.530595421791077\n",
      "Epoch 148 finished in 76.55704593658447 seconds, Average Loss: 6.529328366120656\n",
      "Epoch 149 finished in 76.86678075790405 seconds, Average Loss: 6.52944811185201\n",
      "Epoch 150 finished in 77.68813109397888 seconds, Average Loss: 6.528932114442189\n",
      "Epoch 151 finished in 78.18458199501038 seconds, Average Loss: 6.528951664765676\n",
      "Epoch 152 finished in 77.23554110527039 seconds, Average Loss: 6.528191884358724\n",
      "Epoch 153 finished in 76.52759909629822 seconds, Average Loss: 6.528412997722626\n",
      "Epoch 154 finished in 77.16458702087402 seconds, Average Loss: 6.5297432740529375\n",
      "Epoch 155 finished in 76.779629945755 seconds, Average Loss: 6.528920988241832\n",
      "Epoch 156 finished in 76.42467188835144 seconds, Average Loss: 6.526990274588267\n",
      "Epoch 157 finished in 78.81377625465393 seconds, Average Loss: 6.528126835823059\n",
      "Epoch 158 finished in 78.24546504020691 seconds, Average Loss: 6.526809672514598\n",
      "Epoch 159 finished in 77.8039698600769 seconds, Average Loss: 6.527607282002767\n",
      "Epoch 160 finished in 76.88141703605652 seconds, Average Loss: 6.5280266006787615\n",
      "Epoch 161 finished in 79.88176202774048 seconds, Average Loss: 6.526042898495992\n",
      "Epoch 162 finished in 84.10135698318481 seconds, Average Loss: 6.5278827746709185\n",
      "Epoch 163 finished in 79.87924408912659 seconds, Average Loss: 6.529526889324188\n",
      "Epoch 164 finished in 81.14913177490234 seconds, Average Loss: 6.526591122150421\n",
      "Epoch 165 finished in 81.52995729446411 seconds, Average Loss: 6.526291886965434\n",
      "Epoch 166 finished in 81.71127676963806 seconds, Average Loss: 6.525204877058665\n",
      "Epoch 167 finished in 80.48291993141174 seconds, Average Loss: 6.529887596766154\n",
      "Epoch 168 finished in 77.26445293426514 seconds, Average Loss: 6.526023288567861\n",
      "Epoch 169 finished in 78.01818084716797 seconds, Average Loss: 6.52560019493103\n",
      "Epoch 170 finished in 79.58712697029114 seconds, Average Loss: 6.526161293188731\n",
      "Epoch 171 finished in 78.35466384887695 seconds, Average Loss: 6.526274462540944\n",
      "Epoch 172 finished in 83.7084469795227 seconds, Average Loss: 6.525593082110087\n",
      "Epoch 173 finished in 78.2235119342804 seconds, Average Loss: 6.526883224646251\n",
      "Epoch 174 finished in 79.39061403274536 seconds, Average Loss: 6.526176432768504\n",
      "Epoch 175 finished in 80.53577876091003 seconds, Average Loss: 6.525966982046763\n",
      "Epoch 176 finished in 83.96891188621521 seconds, Average Loss: 6.525665660699208\n",
      "Epoch 177 finished in 79.67402219772339 seconds, Average Loss: 6.526634236176808\n",
      "Epoch 178 finished in 79.69815015792847 seconds, Average Loss: 6.526435673236847\n",
      "Epoch 179 finished in 80.75327897071838 seconds, Average Loss: 6.525738298892975\n",
      "Epoch 180 finished in 79.88717484474182 seconds, Average Loss: 6.525864283243815\n",
      "Epoch 181 finished in 80.91956901550293 seconds, Average Loss: 6.525525013605754\n",
      "Epoch 182 finished in 80.45298171043396 seconds, Average Loss: 6.524486323197682\n",
      "Epoch 183 finished in 80.360910654068 seconds, Average Loss: 6.525006592273712\n",
      "Epoch 184 finished in 80.09324502944946 seconds, Average Loss: 6.524690945943196\n",
      "Epoch 185 finished in 90.67764091491699 seconds, Average Loss: 6.525000154972076\n",
      "Epoch 186 finished in 79.63633322715759 seconds, Average Loss: 6.524768789609273\n",
      "Epoch 187 finished in 79.02941274642944 seconds, Average Loss: 6.524991929531097\n",
      "Epoch 188 finished in 79.23153614997864 seconds, Average Loss: 6.5253409544626875\n",
      "Epoch 189 finished in 80.04385375976562 seconds, Average Loss: 6.524944007396698\n",
      "Epoch 190 finished in 79.49283623695374 seconds, Average Loss: 6.52495676279068\n",
      "Epoch 191 finished in 79.66318893432617 seconds, Average Loss: 6.524146278699239\n",
      "Epoch 192 finished in 76.67015194892883 seconds, Average Loss: 6.5261343121528625\n",
      "Epoch 193 finished in 78.14427876472473 seconds, Average Loss: 6.523316502571106\n",
      "Epoch 194 finished in 77.56937217712402 seconds, Average Loss: 6.524775246779124\n",
      "Epoch 195 finished in 81.29352903366089 seconds, Average Loss: 6.523558060328166\n",
      "Epoch 196 finished in 81.39107823371887 seconds, Average Loss: 6.525205492973328\n",
      "Epoch 197 finished in 80.18818402290344 seconds, Average Loss: 6.526174207528432\n",
      "Epoch 198 finished in 79.80294132232666 seconds, Average Loss: 6.5227983593940735\n",
      "Epoch 199 finished in 79.0819411277771 seconds, Average Loss: 6.525093694527944\n",
      "Epoch 200 finished in 83.12478184700012 seconds, Average Loss: 6.524664461612701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102138410279491e9a33403dc53e32a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.52466</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fluent-glitter-13</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/yxic3ujx' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/yxic3ujx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_063452-yxic3ujx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240326_105815-gjyocpkm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/gjyocpkm' target=\"_blank\">worldly-firefly-14</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/gjyocpkm' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/gjyocpkm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 59.40078401565552 seconds, Average Loss: 7.666528304417928\n",
      "Epoch 2 finished in 58.28634285926819 seconds, Average Loss: 7.187915802001953\n",
      "Epoch 3 finished in 58.58825087547302 seconds, Average Loss: 7.01009076833725\n",
      "Epoch 4 finished in 58.07065510749817 seconds, Average Loss: 6.9075372616449995\n",
      "Epoch 5 finished in 57.965107917785645 seconds, Average Loss: 6.845897654692332\n",
      "Epoch 6 finished in 59.03634810447693 seconds, Average Loss: 6.805721978346507\n",
      "Epoch 7 finished in 59.7060809135437 seconds, Average Loss: 6.779391229152679\n",
      "Epoch 8 finished in 60.85646176338196 seconds, Average Loss: 6.756242334842682\n",
      "Epoch 9 finished in 60.74258303642273 seconds, Average Loss: 6.736295759677887\n",
      "Epoch 10 finished in 61.50138306617737 seconds, Average Loss: 6.720192809899648\n",
      "Epoch 11 finished in 59.99506711959839 seconds, Average Loss: 6.707371989885966\n",
      "Epoch 12 finished in 60.78270101547241 seconds, Average Loss: 6.693694730599721\n",
      "Epoch 13 finished in 59.64031171798706 seconds, Average Loss: 6.683514058589935\n",
      "Epoch 14 finished in 59.48629379272461 seconds, Average Loss: 6.675197919209798\n",
      "Epoch 15 finished in 59.46168804168701 seconds, Average Loss: 6.665496587753296\n",
      "Epoch 16 finished in 59.70535397529602 seconds, Average Loss: 6.6611214478810625\n",
      "Epoch 17 finished in 58.598512172698975 seconds, Average Loss: 6.656738142172496\n",
      "Epoch 18 finished in 58.95538401603699 seconds, Average Loss: 6.649834990501404\n",
      "Epoch 19 finished in 57.94006323814392 seconds, Average Loss: 6.644918779532115\n",
      "Epoch 20 finished in 58.057281732559204 seconds, Average Loss: 6.6378722588221235\n",
      "Epoch 21 finished in 58.28643298149109 seconds, Average Loss: 6.634480853875478\n",
      "Epoch 22 finished in 59.86599683761597 seconds, Average Loss: 6.630822281042735\n",
      "Epoch 23 finished in 61.21918606758118 seconds, Average Loss: 6.628880202770233\n",
      "Epoch 24 finished in 61.46020460128784 seconds, Average Loss: 6.6223971247673035\n",
      "Epoch 25 finished in 61.01543188095093 seconds, Average Loss: 6.621476332346599\n",
      "Epoch 26 finished in 60.650073289871216 seconds, Average Loss: 6.6173138817151385\n",
      "Epoch 27 finished in 61.81440186500549 seconds, Average Loss: 6.6142476598421736\n",
      "Epoch 28 finished in 59.52484607696533 seconds, Average Loss: 6.612166206041972\n",
      "Epoch 29 finished in 59.55632209777832 seconds, Average Loss: 6.6109917759895325\n",
      "Epoch 30 finished in 60.24359917640686 seconds, Average Loss: 6.607796947161357\n",
      "Epoch 31 finished in 60.395159006118774 seconds, Average Loss: 6.605066160360972\n",
      "Epoch 32 finished in 60.62817192077637 seconds, Average Loss: 6.60268892844518\n",
      "Epoch 33 finished in 59.89666986465454 seconds, Average Loss: 6.60270627339681\n",
      "Epoch 34 finished in 61.59246373176575 seconds, Average Loss: 6.601569434007009\n",
      "Epoch 35 finished in 60.319135904312134 seconds, Average Loss: 6.598642508188884\n",
      "Epoch 36 finished in 59.625656843185425 seconds, Average Loss: 6.5984364946683245\n",
      "Epoch 37 finished in 59.62321090698242 seconds, Average Loss: 6.596613148848216\n",
      "Epoch 38 finished in 59.335578203201294 seconds, Average Loss: 6.594310383001964\n",
      "Epoch 39 finished in 59.17648005485535 seconds, Average Loss: 6.595107177893321\n",
      "Epoch 40 finished in 60.20127296447754 seconds, Average Loss: 6.5918205579121905\n",
      "Epoch 41 finished in 59.92844009399414 seconds, Average Loss: 6.5890300671259565\n",
      "Epoch 42 finished in 61.17827606201172 seconds, Average Loss: 6.589624265829722\n",
      "Epoch 43 finished in 60.796233892440796 seconds, Average Loss: 6.5875333944956465\n",
      "Epoch 44 finished in 59.838545083999634 seconds, Average Loss: 6.58856874704361\n",
      "Epoch 45 finished in 59.7928740978241 seconds, Average Loss: 6.584054112434387\n",
      "Epoch 46 finished in 60.30629301071167 seconds, Average Loss: 6.583122571309407\n",
      "Epoch 47 finished in 60.164835929870605 seconds, Average Loss: 6.581757207711537\n",
      "Epoch 48 finished in 59.41847014427185 seconds, Average Loss: 6.580442746480306\n",
      "Epoch 49 finished in 60.48876690864563 seconds, Average Loss: 6.579557339350383\n",
      "Epoch 50 finished in 61.14375400543213 seconds, Average Loss: 6.580627918243408\n",
      "Epoch 51 finished in 61.77075695991516 seconds, Average Loss: 6.578821798165639\n",
      "Epoch 52 finished in 66.05999302864075 seconds, Average Loss: 6.577595015366872\n",
      "Epoch 53 finished in 60.613492012023926 seconds, Average Loss: 6.5771216948827105\n",
      "Epoch 54 finished in 60.39592003822327 seconds, Average Loss: 6.5755049387613935\n",
      "Epoch 55 finished in 62.26243829727173 seconds, Average Loss: 6.578531662623088\n",
      "Epoch 56 finished in 61.55610489845276 seconds, Average Loss: 6.574660817782084\n",
      "Epoch 57 finished in 62.04792380332947 seconds, Average Loss: 6.5752473672231035\n",
      "Epoch 58 finished in 63.13151693344116 seconds, Average Loss: 6.572025756041209\n",
      "Epoch 59 finished in 62.326910972595215 seconds, Average Loss: 6.574323972066243\n",
      "Epoch 60 finished in 61.75261306762695 seconds, Average Loss: 6.569600403308868\n",
      "Epoch 61 finished in 61.9645140171051 seconds, Average Loss: 6.569845378398895\n",
      "Epoch 62 finished in 62.263882875442505 seconds, Average Loss: 6.570909202098846\n",
      "Epoch 63 finished in 61.75274586677551 seconds, Average Loss: 6.568429172039032\n",
      "Epoch 64 finished in 60.508960008621216 seconds, Average Loss: 6.569017251332601\n",
      "Epoch 65 finished in 60.7769992351532 seconds, Average Loss: 6.567696054776509\n",
      "Epoch 66 finished in 60.80397009849548 seconds, Average Loss: 6.567685047785441\n",
      "Epoch 67 finished in 61.57417917251587 seconds, Average Loss: 6.565559685230255\n",
      "Epoch 68 finished in 60.82543325424194 seconds, Average Loss: 6.565358320871989\n",
      "Epoch 69 finished in 61.608617305755615 seconds, Average Loss: 6.564272006352742\n",
      "Epoch 70 finished in 59.79736375808716 seconds, Average Loss: 6.5641898314158125\n",
      "Epoch 71 finished in 60.0149941444397 seconds, Average Loss: 6.564520637194316\n",
      "Epoch 72 finished in 60.68125796318054 seconds, Average Loss: 6.563497066497803\n",
      "Epoch 73 finished in 60.7211217880249 seconds, Average Loss: 6.562391062577565\n",
      "Epoch 74 finished in 61.17025399208069 seconds, Average Loss: 6.562621672948201\n",
      "Epoch 75 finished in 62.370018005371094 seconds, Average Loss: 6.563857873280843\n",
      "Epoch 76 finished in 61.37174081802368 seconds, Average Loss: 6.560571173826854\n",
      "Epoch 77 finished in 61.269540786743164 seconds, Average Loss: 6.561987618605296\n",
      "Epoch 78 finished in 64.9588828086853 seconds, Average Loss: 6.560365895430247\n",
      "Epoch 79 finished in 61.01253890991211 seconds, Average Loss: 6.558543761571248\n",
      "Epoch 80 finished in 60.53890109062195 seconds, Average Loss: 6.559252679347992\n",
      "Epoch 81 finished in 62.794485092163086 seconds, Average Loss: 6.559486567974091\n",
      "Epoch 82 finished in 63.7813937664032 seconds, Average Loss: 6.558080732822418\n",
      "Epoch 83 finished in 61.25430202484131 seconds, Average Loss: 6.557406425476074\n",
      "Epoch 84 finished in 60.93946695327759 seconds, Average Loss: 6.5579632719357805\n",
      "Epoch 85 finished in 60.772526025772095 seconds, Average Loss: 6.558128972848256\n",
      "Epoch 86 finished in 61.32895803451538 seconds, Average Loss: 6.557523548603058\n",
      "Epoch 87 finished in 60.712153911590576 seconds, Average Loss: 6.556500792503357\n",
      "Epoch 88 finished in 60.87114095687866 seconds, Average Loss: 6.555437604586284\n",
      "Epoch 89 finished in 60.71432900428772 seconds, Average Loss: 6.554584542910258\n",
      "Epoch 90 finished in 61.53575801849365 seconds, Average Loss: 6.555030604203542\n",
      "Epoch 91 finished in 61.691543102264404 seconds, Average Loss: 6.554948429266612\n",
      "Epoch 92 finished in 62.10514521598816 seconds, Average Loss: 6.552809317906697\n",
      "Epoch 93 finished in 61.99927806854248 seconds, Average Loss: 6.552422026793162\n",
      "Epoch 94 finished in 60.181193113327026 seconds, Average Loss: 6.5535736083984375\n",
      "Epoch 95 finished in 60.58406972885132 seconds, Average Loss: 6.55252210299174\n",
      "Epoch 96 finished in 59.60716676712036 seconds, Average Loss: 6.551668226718903\n",
      "Epoch 97 finished in 59.75978708267212 seconds, Average Loss: 6.552025536696116\n",
      "Epoch 98 finished in 60.53031086921692 seconds, Average Loss: 6.548891246318817\n",
      "Epoch 99 finished in 60.48440194129944 seconds, Average Loss: 6.550484160582225\n",
      "Epoch 100 finished in 60.16981506347656 seconds, Average Loss: 6.550836602846782\n",
      "Epoch 101 finished in 61.68328380584717 seconds, Average Loss: 6.5502235889434814\n",
      "Epoch 102 finished in 61.556578159332275 seconds, Average Loss: 6.550427854061127\n",
      "Epoch 103 finished in 61.07531404495239 seconds, Average Loss: 6.547804653644562\n",
      "Epoch 104 finished in 61.11600112915039 seconds, Average Loss: 6.548405687014262\n",
      "Epoch 105 finished in 61.92154383659363 seconds, Average Loss: 6.549138863881429\n",
      "Epoch 106 finished in 61.275540828704834 seconds, Average Loss: 6.547427872816722\n",
      "Epoch 107 finished in 59.760125160217285 seconds, Average Loss: 6.549009442329407\n",
      "Epoch 108 finished in 59.73528289794922 seconds, Average Loss: 6.545426547527313\n",
      "Epoch 109 finished in 59.98416209220886 seconds, Average Loss: 6.545317629973094\n",
      "Epoch 110 finished in 59.3047878742218 seconds, Average Loss: 6.547106186548869\n",
      "Epoch 111 finished in 58.97816610336304 seconds, Average Loss: 6.548400441805522\n",
      "Epoch 112 finished in 59.617696046829224 seconds, Average Loss: 6.543562412261963\n",
      "Epoch 113 finished in 60.3963348865509 seconds, Average Loss: 6.544225037097931\n",
      "Epoch 114 finished in 60.62976312637329 seconds, Average Loss: 6.546024779478709\n",
      "Epoch 115 finished in 66.87900304794312 seconds, Average Loss: 6.544359306494395\n",
      "Epoch 116 finished in 62.72957682609558 seconds, Average Loss: 6.545681734879811\n",
      "Epoch 117 finished in 59.47864818572998 seconds, Average Loss: 6.545382142066956\n",
      "Epoch 118 finished in 77.50950908660889 seconds, Average Loss: 6.545495311419169\n",
      "Epoch 119 finished in 59.94366502761841 seconds, Average Loss: 6.544277886549632\n",
      "Epoch 120 finished in 61.711631059646606 seconds, Average Loss: 6.544042885303497\n",
      "Epoch 121 finished in 64.33602905273438 seconds, Average Loss: 6.543335874875386\n",
      "Epoch 122 finished in 61.55488991737366 seconds, Average Loss: 6.54239292939504\n",
      "Epoch 123 finished in 62.74028491973877 seconds, Average Loss: 6.542114953200023\n",
      "Epoch 124 finished in 61.72731328010559 seconds, Average Loss: 6.5436697999636335\n",
      "Epoch 125 finished in 61.091391801834106 seconds, Average Loss: 6.542828440666199\n",
      "Epoch 126 finished in 67.25541400909424 seconds, Average Loss: 6.542202333609263\n",
      "Epoch 127 finished in 61.429829835891724 seconds, Average Loss: 6.542913595835368\n",
      "Epoch 128 finished in 60.79527997970581 seconds, Average Loss: 6.54246590534846\n",
      "Epoch 129 finished in 60.29662728309631 seconds, Average Loss: 6.54156498114268\n",
      "Epoch 130 finished in 60.40174603462219 seconds, Average Loss: 6.540122389793396\n",
      "Epoch 131 finished in 59.45337986946106 seconds, Average Loss: 6.541027228037517\n",
      "Epoch 132 finished in 60.617159843444824 seconds, Average Loss: 6.5394285917282104\n",
      "Epoch 133 finished in 58.65383982658386 seconds, Average Loss: 6.538462082544963\n",
      "Epoch 134 finished in 59.54981780052185 seconds, Average Loss: 6.538442591826121\n",
      "Epoch 135 finished in 59.94834804534912 seconds, Average Loss: 6.538690209388733\n",
      "Epoch 136 finished in 58.99156093597412 seconds, Average Loss: 6.537986159324646\n",
      "Epoch 137 finished in 59.6660099029541 seconds, Average Loss: 6.539480070273082\n",
      "Epoch 138 finished in 61.189860105514526 seconds, Average Loss: 6.539344370365143\n",
      "Epoch 139 finished in 60.81042289733887 seconds, Average Loss: 6.537501414616902\n",
      "Epoch 140 finished in 61.86474585533142 seconds, Average Loss: 6.538407941659291\n",
      "Epoch 141 finished in 60.026113986968994 seconds, Average Loss: 6.537427842617035\n",
      "Epoch 142 finished in 132.37218189239502 seconds, Average Loss: 6.536811053752899\n",
      "Epoch 143 finished in 60.89200305938721 seconds, Average Loss: 6.53657191991806\n",
      "Epoch 144 finished in 58.41215991973877 seconds, Average Loss: 6.535549441973369\n",
      "Epoch 145 finished in 58.300968170166016 seconds, Average Loss: 6.536548336346944\n",
      "Epoch 146 finished in 61.694863080978394 seconds, Average Loss: 6.535483102003734\n",
      "Epoch 147 finished in 65.909423828125 seconds, Average Loss: 6.535315632820129\n",
      "Epoch 148 finished in 64.8909969329834 seconds, Average Loss: 6.536103387673696\n",
      "Epoch 149 finished in 62.71626901626587 seconds, Average Loss: 6.534213284651439\n",
      "Epoch 150 finished in 61.238755226135254 seconds, Average Loss: 6.53674731651942\n",
      "Epoch 151 finished in 59.87269997596741 seconds, Average Loss: 6.5363220771153765\n",
      "Epoch 152 finished in 59.37390208244324 seconds, Average Loss: 6.534988383452098\n",
      "Epoch 153 finished in 59.425034284591675 seconds, Average Loss: 6.535094241301219\n",
      "Epoch 154 finished in 60.2400119304657 seconds, Average Loss: 6.533415337403615\n",
      "Epoch 155 finished in 59.81853008270264 seconds, Average Loss: 6.533746361732483\n",
      "Epoch 156 finished in 59.75777006149292 seconds, Average Loss: 6.535417795181274\n",
      "Epoch 157 finished in 60.129507064819336 seconds, Average Loss: 6.533073385556539\n",
      "Epoch 158 finished in 60.04818415641785 seconds, Average Loss: 6.535171329975128\n",
      "Epoch 159 finished in 60.31007504463196 seconds, Average Loss: 6.533874015013377\n",
      "Epoch 160 finished in 59.91253304481506 seconds, Average Loss: 6.536378999551137\n",
      "Epoch 161 finished in 59.71077489852905 seconds, Average Loss: 6.535492082436879\n",
      "Epoch 162 finished in 60.084233045578 seconds, Average Loss: 6.533786296844482\n",
      "Epoch 163 finished in 74.04462909698486 seconds, Average Loss: 6.53287273645401\n",
      "Epoch 164 finished in 71.351970911026 seconds, Average Loss: 6.533349672953288\n",
      "Epoch 165 finished in 62.72588229179382 seconds, Average Loss: 6.534359415372212\n",
      "Epoch 166 finished in 63.288339138031006 seconds, Average Loss: 6.534213960170746\n",
      "Epoch 167 finished in 84.89927411079407 seconds, Average Loss: 6.532516161600749\n",
      "Epoch 168 finished in 64.44328188896179 seconds, Average Loss: 6.5322436690330505\n",
      "Epoch 169 finished in 64.3148078918457 seconds, Average Loss: 6.533106823762258\n",
      "Epoch 170 finished in 64.07090878486633 seconds, Average Loss: 6.5334198872248335\n",
      "Epoch 171 finished in 64.3972430229187 seconds, Average Loss: 6.533896903196971\n",
      "Epoch 172 finished in 61.61722707748413 seconds, Average Loss: 6.532794733842214\n",
      "Epoch 173 finished in 92.58524894714355 seconds, Average Loss: 6.532863438129425\n",
      "Epoch 174 finished in 61.38829493522644 seconds, Average Loss: 6.5324498017628985\n",
      "Epoch 175 finished in 63.51275706291199 seconds, Average Loss: 6.533527831236522\n",
      "Epoch 176 finished in 69.60961985588074 seconds, Average Loss: 6.532186170419057\n",
      "Epoch 177 finished in 76.63085961341858 seconds, Average Loss: 6.533198575178782\n",
      "Epoch 178 finished in 75.28879284858704 seconds, Average Loss: 6.532060861587524\n",
      "Epoch 179 finished in 66.37055993080139 seconds, Average Loss: 6.533783356348674\n",
      "Epoch 180 finished in 74.97627234458923 seconds, Average Loss: 6.532489776611328\n",
      "Epoch 181 finished in 64.43380570411682 seconds, Average Loss: 6.533578236897786\n",
      "Epoch 182 finished in 61.492758989334106 seconds, Average Loss: 6.532340963681539\n",
      "Epoch 183 finished in 61.45090103149414 seconds, Average Loss: 6.532024443149567\n",
      "Epoch 184 finished in 59.92198991775513 seconds, Average Loss: 6.531480054060618\n",
      "Epoch 185 finished in 58.278908252716064 seconds, Average Loss: 6.533871392409007\n",
      "Epoch 186 finished in 58.267688035964966 seconds, Average Loss: 6.532398422559102\n",
      "Epoch 187 finished in 58.058290243148804 seconds, Average Loss: 6.531734486420949\n",
      "Epoch 188 finished in 57.77331566810608 seconds, Average Loss: 6.531263093153636\n",
      "Epoch 189 finished in 58.18548798561096 seconds, Average Loss: 6.531401038169861\n",
      "Epoch 190 finished in 57.80365610122681 seconds, Average Loss: 6.531429588794708\n",
      "Epoch 191 finished in 58.36527991294861 seconds, Average Loss: 6.532225648562114\n",
      "Epoch 192 finished in 1051.8411111831665 seconds, Average Loss: 6.531864543755849\n",
      "Epoch 193 finished in 62.50584292411804 seconds, Average Loss: 6.53183388710022\n",
      "Epoch 194 finished in 58.83062505722046 seconds, Average Loss: 6.531379143397014\n",
      "Epoch 195 finished in 60.0796799659729 seconds, Average Loss: 6.531441609064738\n",
      "Epoch 196 finished in 78.39555406570435 seconds, Average Loss: 6.5305976668993635\n",
      "Epoch 197 finished in 67.50025129318237 seconds, Average Loss: 6.532307147979736\n",
      "Epoch 198 finished in 64.18840599060059 seconds, Average Loss: 6.532044609387715\n",
      "Epoch 199 finished in 68.97574281692505 seconds, Average Loss: 6.531710624694824\n",
      "Epoch 200 finished in 73.51518702507019 seconds, Average Loss: 6.531761626402537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ab361a6fd44e60ad413c8e5a39ffd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.53176</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-firefly-14</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/gjyocpkm' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/gjyocpkm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_105815-gjyocpkm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240326_144205-ttdtk5nw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ttdtk5nw' target=\"_blank\">helpful-leaf-15</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ttdtk5nw' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ttdtk5nw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 168.95126914978027 seconds, Average Loss: 7.7206191420555115\n",
      "Epoch 2 finished in 155.82712411880493 seconds, Average Loss: 7.209270636240642\n",
      "Epoch 3 finished in 141.6147198677063 seconds, Average Loss: 7.00777268409729\n",
      "Epoch 4 finished in 159.22644186019897 seconds, Average Loss: 6.902894715468089\n",
      "Epoch 5 finished in 152.84524488449097 seconds, Average Loss: 6.839805046717326\n",
      "Epoch 6 finished in 150.31048917770386 seconds, Average Loss: 6.799568255742391\n",
      "Epoch 7 finished in 154.7208321094513 seconds, Average Loss: 6.771305282910665\n",
      "Epoch 8 finished in 147.2647430896759 seconds, Average Loss: 6.745318313439687\n",
      "Epoch 9 finished in 149.5320246219635 seconds, Average Loss: 6.728159685929616\n",
      "Epoch 10 finished in 144.3449981212616 seconds, Average Loss: 6.7134332458178205\n",
      "Epoch 11 finished in 143.3980541229248 seconds, Average Loss: 6.6984557112058\n",
      "Epoch 12 finished in 144.52461290359497 seconds, Average Loss: 6.688430110613505\n",
      "Epoch 13 finished in 147.45845818519592 seconds, Average Loss: 6.67704043785731\n",
      "Epoch 14 finished in 152.7461609840393 seconds, Average Loss: 6.670840462048848\n",
      "Epoch 15 finished in 143.88175892829895 seconds, Average Loss: 6.663380602995555\n",
      "Epoch 16 finished in 143.83946013450623 seconds, Average Loss: 6.657199223836263\n",
      "Epoch 17 finished in 157.66868901252747 seconds, Average Loss: 6.652943054835002\n",
      "Epoch 18 finished in 144.41701912879944 seconds, Average Loss: 6.648887832959493\n",
      "Epoch 19 finished in 145.9881489276886 seconds, Average Loss: 6.643238604068756\n",
      "Epoch 20 finished in 145.55980277061462 seconds, Average Loss: 6.638125419616699\n",
      "Epoch 21 finished in 158.2287619113922 seconds, Average Loss: 6.634614686171214\n",
      "Epoch 22 finished in 146.83938026428223 seconds, Average Loss: 6.628939588864644\n",
      "Epoch 23 finished in 177.2967391014099 seconds, Average Loss: 6.627449870109558\n",
      "Epoch 24 finished in 145.76097464561462 seconds, Average Loss: 6.624019364515941\n",
      "Epoch 25 finished in 147.1061191558838 seconds, Average Loss: 6.620242516199748\n",
      "Epoch 26 finished in 144.34809970855713 seconds, Average Loss: 6.616232514381409\n",
      "Epoch 27 finished in 141.09521293640137 seconds, Average Loss: 6.616494297981262\n",
      "Epoch 28 finished in 140.74089288711548 seconds, Average Loss: 6.612146993478139\n",
      "Epoch 29 finished in 143.17930817604065 seconds, Average Loss: 6.609687983989716\n",
      "Epoch 30 finished in 143.85534715652466 seconds, Average Loss: 6.609640081723531\n",
      "Epoch 31 finished in 149.2515959739685 seconds, Average Loss: 6.604660193125407\n",
      "Epoch 32 finished in 139.16847705841064 seconds, Average Loss: 6.603567838668823\n",
      "Epoch 33 finished in 140.7326738834381 seconds, Average Loss: 6.60222323735555\n",
      "Epoch 34 finished in 140.29594588279724 seconds, Average Loss: 6.6008203228314715\n",
      "Epoch 35 finished in 140.82235383987427 seconds, Average Loss: 6.599496424198151\n",
      "Epoch 36 finished in 139.98559093475342 seconds, Average Loss: 6.597564140955607\n",
      "Epoch 37 finished in 140.54746985435486 seconds, Average Loss: 6.596031506856282\n",
      "Epoch 38 finished in 137.41996312141418 seconds, Average Loss: 6.594020863374074\n",
      "Epoch 39 finished in 138.69050121307373 seconds, Average Loss: 6.591123441855113\n",
      "Epoch 40 finished in 134.08757400512695 seconds, Average Loss: 6.58955442905426\n",
      "Epoch 41 finished in 146.3164300918579 seconds, Average Loss: 6.591335992018382\n",
      "Epoch 42 finished in 136.33671689033508 seconds, Average Loss: 6.589536587397258\n",
      "Epoch 43 finished in 137.011727809906 seconds, Average Loss: 6.588073174158732\n",
      "Epoch 44 finished in 136.60640811920166 seconds, Average Loss: 6.586173514525096\n",
      "Epoch 45 finished in 137.17241883277893 seconds, Average Loss: 6.586378912130992\n",
      "Epoch 46 finished in 139.897873878479 seconds, Average Loss: 6.58365269502004\n",
      "Epoch 47 finished in 139.27281379699707 seconds, Average Loss: 6.5853574474652605\n",
      "Epoch 48 finished in 141.80182576179504 seconds, Average Loss: 6.583750764528911\n",
      "Epoch 49 finished in 141.6553258895874 seconds, Average Loss: 6.578827818234761\n",
      "Epoch 50 finished in 143.72857999801636 seconds, Average Loss: 6.580146789550781\n",
      "Epoch 51 finished in 140.1586399078369 seconds, Average Loss: 6.578122874101003\n",
      "Epoch 52 finished in 141.78814101219177 seconds, Average Loss: 6.5777507821718855\n",
      "Epoch 53 finished in 139.40211820602417 seconds, Average Loss: 6.5768934686978655\n",
      "Epoch 54 finished in 144.06829380989075 seconds, Average Loss: 6.5773905118306475\n",
      "Epoch 55 finished in 144.2065920829773 seconds, Average Loss: 6.57446010907491\n",
      "Epoch 56 finished in 154.2205777168274 seconds, Average Loss: 6.573862314224243\n",
      "Epoch 57 finished in 140.22139716148376 seconds, Average Loss: 6.573589722315471\n",
      "Epoch 58 finished in 139.18777179718018 seconds, Average Loss: 6.571089247862498\n",
      "Epoch 59 finished in 140.55936884880066 seconds, Average Loss: 6.571982125441234\n",
      "Epoch 60 finished in 146.12789011001587 seconds, Average Loss: 6.571758151054382\n",
      "Epoch 61 finished in 140.50898694992065 seconds, Average Loss: 6.570972760518392\n",
      "Epoch 62 finished in 135.609032869339 seconds, Average Loss: 6.5697146852811175\n",
      "Epoch 63 finished in 136.73272013664246 seconds, Average Loss: 6.570214529832204\n",
      "Epoch 64 finished in 137.4346010684967 seconds, Average Loss: 6.569718400637309\n",
      "Epoch 65 finished in 135.26208090782166 seconds, Average Loss: 6.5660391847292585\n",
      "Epoch 66 finished in 141.5838921070099 seconds, Average Loss: 6.566730360190074\n",
      "Epoch 67 finished in 138.61413621902466 seconds, Average Loss: 6.566142578919728\n",
      "Epoch 68 finished in 141.17425203323364 seconds, Average Loss: 6.565126220385234\n",
      "Epoch 69 finished in 143.69820618629456 seconds, Average Loss: 6.565351108709971\n",
      "Epoch 70 finished in 141.0704083442688 seconds, Average Loss: 6.564193685849507\n",
      "Epoch 71 finished in 140.8120460510254 seconds, Average Loss: 6.562843501567841\n",
      "Epoch 72 finished in 141.72807002067566 seconds, Average Loss: 6.564684291680654\n",
      "Epoch 73 finished in 140.9255747795105 seconds, Average Loss: 6.561213870843251\n",
      "Epoch 74 finished in 139.8694019317627 seconds, Average Loss: 6.5618003606796265\n",
      "Epoch 75 finished in 141.71208500862122 seconds, Average Loss: 6.560842613379161\n",
      "Epoch 76 finished in 144.15466690063477 seconds, Average Loss: 6.56041278441747\n",
      "Epoch 77 finished in 137.38430285453796 seconds, Average Loss: 6.5607525904973345\n",
      "Epoch 78 finished in 140.59897375106812 seconds, Average Loss: 6.56121168533961\n",
      "Epoch 79 finished in 142.2713258266449 seconds, Average Loss: 6.559010108311971\n",
      "Epoch 80 finished in 141.6343128681183 seconds, Average Loss: 6.557622293631236\n",
      "Epoch 81 finished in 138.80812191963196 seconds, Average Loss: 6.558435022830963\n",
      "Epoch 82 finished in 138.99409699440002 seconds, Average Loss: 6.560348808765411\n",
      "Epoch 83 finished in 139.78561091423035 seconds, Average Loss: 6.555397629737854\n",
      "Epoch 84 finished in 141.9169192314148 seconds, Average Loss: 6.557182272275289\n",
      "Epoch 85 finished in 137.7012391090393 seconds, Average Loss: 6.557506759961446\n",
      "Epoch 86 finished in 138.9513726234436 seconds, Average Loss: 6.556319375832875\n",
      "Epoch 87 finished in 137.68625903129578 seconds, Average Loss: 6.556228935718536\n",
      "Epoch 88 finished in 140.62996578216553 seconds, Average Loss: 6.557170987129211\n",
      "Epoch 89 finished in 148.8744170665741 seconds, Average Loss: 6.556671619415283\n",
      "Epoch 90 finished in 139.91020512580872 seconds, Average Loss: 6.554399947325389\n",
      "Epoch 91 finished in 139.48564887046814 seconds, Average Loss: 6.553392191727956\n",
      "Epoch 92 finished in 142.23488783836365 seconds, Average Loss: 6.552260160446167\n",
      "Epoch 93 finished in 137.18576502799988 seconds, Average Loss: 6.552344044049581\n",
      "Epoch 94 finished in 138.9707510471344 seconds, Average Loss: 6.5529957214991255\n",
      "Epoch 95 finished in 141.65465593338013 seconds, Average Loss: 6.55196479956309\n",
      "Epoch 96 finished in 138.16646218299866 seconds, Average Loss: 6.552812457084656\n",
      "Epoch 97 finished in 145.7149429321289 seconds, Average Loss: 6.550245503584544\n",
      "Epoch 98 finished in 139.77748203277588 seconds, Average Loss: 6.5507651170094805\n",
      "Epoch 99 finished in 140.6180579662323 seconds, Average Loss: 6.552298645178477\n",
      "Epoch 100 finished in 138.66930413246155 seconds, Average Loss: 6.5501716534296675\n",
      "Epoch 101 finished in 140.36398220062256 seconds, Average Loss: 6.549269020557404\n",
      "Epoch 102 finished in 147.0529270172119 seconds, Average Loss: 6.548269867897034\n",
      "Epoch 103 finished in 138.632169008255 seconds, Average Loss: 6.54759007692337\n",
      "Epoch 104 finished in 140.7900071144104 seconds, Average Loss: 6.548938910166423\n",
      "Epoch 105 finished in 177.7734830379486 seconds, Average Loss: 6.549597005049388\n",
      "Epoch 106 finished in 171.3780379295349 seconds, Average Loss: 6.548352797826131\n",
      "Epoch 107 finished in 140.91496992111206 seconds, Average Loss: 6.547506312529246\n",
      "Epoch 108 finished in 148.19730615615845 seconds, Average Loss: 6.547394335269928\n",
      "Epoch 109 finished in 137.798193693161 seconds, Average Loss: 6.5474669734636946\n",
      "Epoch 110 finished in 141.61598825454712 seconds, Average Loss: 6.546188990275065\n",
      "Epoch 111 finished in 143.19133305549622 seconds, Average Loss: 6.545927306016286\n",
      "Epoch 112 finished in 141.00725722312927 seconds, Average Loss: 6.546503285566966\n",
      "Epoch 113 finished in 138.61380195617676 seconds, Average Loss: 6.545501708984375\n",
      "Epoch 114 finished in 139.95752906799316 seconds, Average Loss: 6.544867515563965\n",
      "Epoch 115 finished in 139.23451113700867 seconds, Average Loss: 6.5444562236468\n",
      "Epoch 116 finished in 140.95235586166382 seconds, Average Loss: 6.543959081172943\n",
      "Epoch 117 finished in 140.36666011810303 seconds, Average Loss: 6.544838726520538\n",
      "Epoch 118 finished in 139.25702786445618 seconds, Average Loss: 6.542169352372487\n",
      "Epoch 119 finished in 138.6268470287323 seconds, Average Loss: 6.541549642880757\n",
      "Epoch 120 finished in 138.68502497673035 seconds, Average Loss: 6.541555643081665\n",
      "Epoch 121 finished in 140.03667187690735 seconds, Average Loss: 6.5427801211675005\n",
      "Epoch 122 finished in 140.87818813323975 seconds, Average Loss: 6.54208105802536\n",
      "Epoch 123 finished in 141.8065221309662 seconds, Average Loss: 6.542236506938934\n",
      "Epoch 124 finished in 138.65063667297363 seconds, Average Loss: 6.541586140791575\n",
      "Epoch 125 finished in 140.27518510818481 seconds, Average Loss: 6.540883839130402\n",
      "Epoch 126 finished in 139.10394072532654 seconds, Average Loss: 6.540983001391093\n",
      "Epoch 127 finished in 141.2136356830597 seconds, Average Loss: 6.540512382984161\n",
      "Epoch 128 finished in 138.1643316745758 seconds, Average Loss: 6.541008313496907\n",
      "Epoch 129 finished in 136.2842948436737 seconds, Average Loss: 6.541321953137715\n",
      "Epoch 130 finished in 136.91883897781372 seconds, Average Loss: 6.542731821537018\n",
      "Epoch 131 finished in 137.27516913414001 seconds, Average Loss: 6.537665645281474\n",
      "Epoch 132 finished in 139.32495999336243 seconds, Average Loss: 6.539545933405559\n",
      "Epoch 133 finished in 139.58718490600586 seconds, Average Loss: 6.540477275848389\n",
      "Epoch 134 finished in 139.02267909049988 seconds, Average Loss: 6.539047718048096\n",
      "Epoch 135 finished in 138.28494906425476 seconds, Average Loss: 6.538397332032521\n",
      "Epoch 136 finished in 139.67287397384644 seconds, Average Loss: 6.538381775220235\n",
      "Epoch 137 finished in 143.31629705429077 seconds, Average Loss: 6.538370370864868\n",
      "Epoch 138 finished in 138.57350778579712 seconds, Average Loss: 6.538869341214498\n",
      "Epoch 139 finished in 139.85367393493652 seconds, Average Loss: 6.536931912104289\n",
      "Epoch 140 finished in 143.16893124580383 seconds, Average Loss: 6.538014928499858\n",
      "Epoch 141 finished in 140.00986576080322 seconds, Average Loss: 6.535142123699188\n",
      "Epoch 142 finished in 139.68291306495667 seconds, Average Loss: 6.53722216685613\n",
      "Epoch 143 finished in 142.22440028190613 seconds, Average Loss: 6.535942097504933\n",
      "Epoch 144 finished in 140.32635116577148 seconds, Average Loss: 6.538229167461395\n",
      "Epoch 145 finished in 140.98231506347656 seconds, Average Loss: 6.536255498727162\n",
      "Epoch 146 finished in 139.8050229549408 seconds, Average Loss: 6.537266433238983\n",
      "Epoch 147 finished in 137.2757031917572 seconds, Average Loss: 6.537855923175812\n",
      "Epoch 148 finished in 141.76247596740723 seconds, Average Loss: 6.536248584588368\n",
      "Epoch 149 finished in 140.88162803649902 seconds, Average Loss: 6.535997807979584\n",
      "Epoch 150 finished in 138.22112011909485 seconds, Average Loss: 6.536389887332916\n",
      "Epoch 151 finished in 140.77061986923218 seconds, Average Loss: 6.534838795661926\n",
      "Epoch 152 finished in 142.63677191734314 seconds, Average Loss: 6.535017311573029\n",
      "Epoch 153 finished in 147.0302028656006 seconds, Average Loss: 6.534075617790222\n",
      "Epoch 154 finished in 142.9278028011322 seconds, Average Loss: 6.533972263336182\n",
      "Epoch 155 finished in 142.4100432395935 seconds, Average Loss: 6.535751779874166\n",
      "Epoch 156 finished in 142.42173290252686 seconds, Average Loss: 6.533949653307597\n",
      "Epoch 157 finished in 138.13507604599 seconds, Average Loss: 6.531034529209137\n",
      "Epoch 158 finished in 139.6698338985443 seconds, Average Loss: 6.533472200234731\n",
      "Epoch 159 finished in 140.1795802116394 seconds, Average Loss: 6.534307221571605\n",
      "Epoch 160 finished in 139.28365278244019 seconds, Average Loss: 6.53439998626709\n",
      "Epoch 161 finished in 138.9975769519806 seconds, Average Loss: 6.53294571240743\n",
      "Epoch 162 finished in 140.45168900489807 seconds, Average Loss: 6.533243993918101\n",
      "Epoch 163 finished in 140.96616291999817 seconds, Average Loss: 6.5335839788119\n",
      "Epoch 164 finished in 140.71833539009094 seconds, Average Loss: 6.5328619082768755\n",
      "Epoch 165 finished in 145.27278470993042 seconds, Average Loss: 6.532992700735728\n",
      "Epoch 166 finished in 137.91545391082764 seconds, Average Loss: 6.533245027065277\n",
      "Epoch 167 finished in 139.0585720539093 seconds, Average Loss: 6.5306223432223005\n",
      "Epoch 168 finished in 139.0532112121582 seconds, Average Loss: 6.532292286554973\n",
      "Epoch 169 finished in 140.01455307006836 seconds, Average Loss: 6.5334420800209045\n",
      "Epoch 170 finished in 142.5867269039154 seconds, Average Loss: 6.533480068047841\n",
      "Epoch 171 finished in 137.65039110183716 seconds, Average Loss: 6.532647212346395\n",
      "Epoch 172 finished in 141.34686303138733 seconds, Average Loss: 6.532271246115367\n",
      "Epoch 173 finished in 142.28556418418884 seconds, Average Loss: 6.531663239002228\n",
      "Epoch 174 finished in 138.9906461238861 seconds, Average Loss: 6.533160249392192\n",
      "Epoch 175 finished in 139.3239369392395 seconds, Average Loss: 6.530566155910492\n",
      "Epoch 176 finished in 141.0970380306244 seconds, Average Loss: 6.532475252946218\n",
      "Epoch 177 finished in 142.5416829586029 seconds, Average Loss: 6.530872941017151\n",
      "Epoch 178 finished in 139.70061898231506 seconds, Average Loss: 6.53083469470342\n",
      "Epoch 179 finished in 140.28186106681824 seconds, Average Loss: 6.530932386716207\n",
      "Epoch 180 finished in 140.32719802856445 seconds, Average Loss: 6.530922253926595\n",
      "Epoch 181 finished in 163.2825689315796 seconds, Average Loss: 6.5308383504549665\n",
      "Epoch 182 finished in 163.5794541835785 seconds, Average Loss: 6.531482060750325\n",
      "Epoch 183 finished in 154.09232187271118 seconds, Average Loss: 6.529583891232808\n",
      "Epoch 184 finished in 138.08037090301514 seconds, Average Loss: 6.531000057856242\n",
      "Epoch 185 finished in 138.71808290481567 seconds, Average Loss: 6.530825098355611\n",
      "Epoch 186 finished in 150.88503098487854 seconds, Average Loss: 6.531258602937062\n",
      "Epoch 187 finished in 150.68553066253662 seconds, Average Loss: 6.532303849856059\n",
      "Epoch 188 finished in 146.77370691299438 seconds, Average Loss: 6.532050549983978\n",
      "Epoch 189 finished in 144.3628740310669 seconds, Average Loss: 6.530552764733632\n",
      "Epoch 190 finished in 143.66950392723083 seconds, Average Loss: 6.529740830262502\n",
      "Epoch 191 finished in 143.90849804878235 seconds, Average Loss: 6.530197421709697\n",
      "Epoch 192 finished in 145.34693789482117 seconds, Average Loss: 6.529919683933258\n",
      "Epoch 193 finished in 143.11470699310303 seconds, Average Loss: 6.532058993975322\n",
      "Epoch 194 finished in 140.8205623626709 seconds, Average Loss: 6.531453192234039\n",
      "Epoch 195 finished in 143.42649483680725 seconds, Average Loss: 6.530969162782033\n",
      "Epoch 196 finished in 147.38179802894592 seconds, Average Loss: 6.530876080195109\n",
      "Epoch 197 finished in 142.64840602874756 seconds, Average Loss: 6.532384534676869\n",
      "Epoch 198 finished in 143.30916571617126 seconds, Average Loss: 6.5326395233472185\n",
      "Epoch 199 finished in 141.8149869441986 seconds, Average Loss: 6.528719405333201\n",
      "Epoch 200 finished in 140.3990957736969 seconds, Average Loss: 6.530369758605957\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8a023e3a394576a287ea75837a54b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.53037</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-leaf-15</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ttdtk5nw' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ttdtk5nw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_144205-ttdtk5nw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240326_223804-8l3ikohf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/8l3ikohf' target=\"_blank\">winter-violet-16</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/8l3ikohf' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/8l3ikohf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 65.19251203536987 seconds, Average Loss: 7.877552390098572\n",
      "Epoch 2 finished in 65.18176412582397 seconds, Average Loss: 7.512333472569783\n",
      "Epoch 3 finished in 67.58386778831482 seconds, Average Loss: 7.378252625465393\n",
      "Epoch 4 finished in 80.57423520088196 seconds, Average Loss: 7.298437535762787\n",
      "Epoch 5 finished in 89.68061709403992 seconds, Average Loss: 7.242946366469066\n",
      "Epoch 6 finished in 78.5651581287384 seconds, Average Loss: 7.199544648329417\n",
      "Epoch 7 finished in 74.99510097503662 seconds, Average Loss: 7.168028354644775\n",
      "Epoch 8 finished in 72.92862701416016 seconds, Average Loss: 7.135475397109985\n",
      "Epoch 9 finished in 73.32662987709045 seconds, Average Loss: 7.112058718999227\n",
      "Epoch 10 finished in 73.6548490524292 seconds, Average Loss: 7.080657382806142\n",
      "Epoch 11 finished in 74.32603597640991 seconds, Average Loss: 7.062133689721425\n",
      "Epoch 12 finished in 73.35431385040283 seconds, Average Loss: 7.042662700017293\n",
      "Epoch 13 finished in 74.29317688941956 seconds, Average Loss: 7.024282077948253\n",
      "Epoch 14 finished in 80.93281602859497 seconds, Average Loss: 7.0077423850695295\n",
      "Epoch 15 finished in 86.52516770362854 seconds, Average Loss: 6.99395626783371\n",
      "Epoch 16 finished in 71.87708902359009 seconds, Average Loss: 6.981983780860901\n",
      "Epoch 17 finished in 70.44119596481323 seconds, Average Loss: 6.97387299935023\n",
      "Epoch 18 finished in 71.18359398841858 seconds, Average Loss: 6.9632634917895\n",
      "Epoch 19 finished in 70.44280219078064 seconds, Average Loss: 6.954033573468526\n",
      "Epoch 20 finished in 71.24206614494324 seconds, Average Loss: 6.942260980606079\n",
      "Epoch 21 finished in 74.20186400413513 seconds, Average Loss: 6.934616406758626\n",
      "Epoch 22 finished in 73.42566704750061 seconds, Average Loss: 6.92762557665507\n",
      "Epoch 23 finished in 73.45722484588623 seconds, Average Loss: 6.922848006089528\n",
      "Epoch 24 finished in 73.98830008506775 seconds, Average Loss: 6.912079135576884\n",
      "Epoch 25 finished in 72.84191417694092 seconds, Average Loss: 6.906767308712006\n",
      "Epoch 26 finished in 71.34776711463928 seconds, Average Loss: 6.9004331429799395\n",
      "Epoch 27 finished in 72.0883960723877 seconds, Average Loss: 6.892011006673177\n",
      "Epoch 28 finished in 69.95288896560669 seconds, Average Loss: 6.892847021420796\n",
      "Epoch 29 finished in 70.73660778999329 seconds, Average Loss: 6.887709339459737\n",
      "Epoch 30 finished in 70.51646995544434 seconds, Average Loss: 6.8762433330218\n",
      "Epoch 31 finished in 70.12254500389099 seconds, Average Loss: 6.870082100232442\n",
      "Epoch 32 finished in 69.14021682739258 seconds, Average Loss: 6.868668615818024\n",
      "Epoch 33 finished in 69.01350235939026 seconds, Average Loss: 6.86595344543457\n",
      "Epoch 34 finished in 70.00733494758606 seconds, Average Loss: 6.863016287485759\n",
      "Epoch 35 finished in 69.12901091575623 seconds, Average Loss: 6.861358304818471\n",
      "Epoch 36 finished in 71.23307704925537 seconds, Average Loss: 6.856538236141205\n",
      "Epoch 37 finished in 69.86560082435608 seconds, Average Loss: 6.847718218962352\n",
      "Epoch 38 finished in 73.86468887329102 seconds, Average Loss: 6.848338743050893\n",
      "Epoch 39 finished in 65.48584985733032 seconds, Average Loss: 6.844116508960724\n",
      "Epoch 40 finished in 64.07591009140015 seconds, Average Loss: 6.839268962542216\n",
      "Epoch 41 finished in 63.81663799285889 seconds, Average Loss: 6.837916115919749\n",
      "Epoch 42 finished in 63.6512451171875 seconds, Average Loss: 6.832995891571045\n",
      "Epoch 43 finished in 63.457151889801025 seconds, Average Loss: 6.8350963195164995\n",
      "Epoch 44 finished in 63.50256395339966 seconds, Average Loss: 6.825414617856343\n",
      "Epoch 45 finished in 63.57516694068909 seconds, Average Loss: 6.825126310189565\n",
      "Epoch 46 finished in 64.9786159992218 seconds, Average Loss: 6.82433698574702\n",
      "Epoch 47 finished in 74.73024320602417 seconds, Average Loss: 6.821799715360005\n",
      "Epoch 48 finished in 63.4449098110199 seconds, Average Loss: 6.816877365112305\n",
      "Epoch 49 finished in 63.571414947509766 seconds, Average Loss: 6.8158761858940125\n",
      "Epoch 50 finished in 63.59785199165344 seconds, Average Loss: 6.8140467802683515\n",
      "Epoch 51 finished in 63.562318086624146 seconds, Average Loss: 6.811428527037303\n",
      "Epoch 52 finished in 63.51935291290283 seconds, Average Loss: 6.810815592606862\n",
      "Epoch 53 finished in 63.56386685371399 seconds, Average Loss: 6.807840486367543\n",
      "Epoch 54 finished in 63.62090802192688 seconds, Average Loss: 6.807577649752299\n",
      "Epoch 55 finished in 63.467129945755005 seconds, Average Loss: 6.802500506242116\n",
      "Epoch 56 finished in 63.556581020355225 seconds, Average Loss: 6.800557196140289\n",
      "Epoch 57 finished in 63.87447929382324 seconds, Average Loss: 6.799776375293732\n",
      "Epoch 58 finished in 63.53015613555908 seconds, Average Loss: 6.796167810757955\n",
      "Epoch 59 finished in 63.5265588760376 seconds, Average Loss: 6.793887734413147\n",
      "Epoch 60 finished in 63.588690996170044 seconds, Average Loss: 6.79807585477829\n",
      "Epoch 61 finished in 63.4403018951416 seconds, Average Loss: 6.788489580154419\n",
      "Epoch 62 finished in 63.50034809112549 seconds, Average Loss: 6.790123999118805\n",
      "Epoch 63 finished in 63.58664417266846 seconds, Average Loss: 6.786771694819133\n",
      "Epoch 64 finished in 63.49905610084534 seconds, Average Loss: 6.788213948408763\n",
      "Epoch 65 finished in 63.454185962677 seconds, Average Loss: 6.787915150324504\n",
      "Epoch 66 finished in 63.57051491737366 seconds, Average Loss: 6.78421421845754\n",
      "Epoch 67 finished in 63.53742694854736 seconds, Average Loss: 6.781174858411153\n",
      "Epoch 68 finished in 63.470858097076416 seconds, Average Loss: 6.784049868583679\n",
      "Epoch 69 finished in 63.477625131607056 seconds, Average Loss: 6.780374805132548\n",
      "Epoch 70 finished in 63.66797685623169 seconds, Average Loss: 6.780041257540385\n",
      "Epoch 71 finished in 63.46799373626709 seconds, Average Loss: 6.780013223489125\n",
      "Epoch 72 finished in 63.57183814048767 seconds, Average Loss: 6.773354550202687\n",
      "Epoch 73 finished in 63.53976583480835 seconds, Average Loss: 6.776801764965057\n",
      "Epoch 74 finished in 63.45908498764038 seconds, Average Loss: 6.774563392003377\n",
      "Epoch 75 finished in 63.45965099334717 seconds, Average Loss: 6.769855201244354\n",
      "Epoch 76 finished in 63.67649006843567 seconds, Average Loss: 6.7732627391815186\n",
      "Epoch 77 finished in 63.53167009353638 seconds, Average Loss: 6.76697313785553\n",
      "Epoch 78 finished in 63.50595188140869 seconds, Average Loss: 6.768014113108317\n",
      "Epoch 79 finished in 63.61507201194763 seconds, Average Loss: 6.765434145927429\n",
      "Epoch 80 finished in 63.5709490776062 seconds, Average Loss: 6.766757706801097\n",
      "Epoch 81 finished in 63.50819683074951 seconds, Average Loss: 6.765622874101003\n",
      "Epoch 82 finished in 63.607120990753174 seconds, Average Loss: 6.761794884999593\n",
      "Epoch 83 finished in 63.576786041259766 seconds, Average Loss: 6.760444064935048\n",
      "Epoch 84 finished in 63.62857699394226 seconds, Average Loss: 6.760190665721893\n",
      "Epoch 85 finished in 63.54394292831421 seconds, Average Loss: 6.760287165641785\n",
      "Epoch 86 finished in 63.817569971084595 seconds, Average Loss: 6.757548650105794\n",
      "Epoch 87 finished in 63.56908321380615 seconds, Average Loss: 6.755590438842773\n",
      "Epoch 88 finished in 63.64744329452515 seconds, Average Loss: 6.75740001598994\n",
      "Epoch 89 finished in 63.704965114593506 seconds, Average Loss: 6.755489508310954\n",
      "Epoch 90 finished in 63.502864837646484 seconds, Average Loss: 6.7527241508166\n",
      "Epoch 91 finished in 63.8381028175354 seconds, Average Loss: 6.754012584686279\n",
      "Epoch 92 finished in 63.837316274642944 seconds, Average Loss: 6.7516939242680865\n",
      "Epoch 93 finished in 63.650619983673096 seconds, Average Loss: 6.750023941198985\n",
      "Epoch 94 finished in 63.516849756240845 seconds, Average Loss: 6.748334030310313\n",
      "Epoch 95 finished in 63.80494284629822 seconds, Average Loss: 6.750570992628734\n",
      "Epoch 96 finished in 63.91265678405762 seconds, Average Loss: 6.747403979301453\n",
      "Epoch 97 finished in 63.62303185462952 seconds, Average Loss: 6.746637582778931\n",
      "Epoch 98 finished in 63.95802903175354 seconds, Average Loss: 6.7487263679504395\n",
      "Epoch 99 finished in 63.71253037452698 seconds, Average Loss: 6.742585102717082\n",
      "Epoch 100 finished in 63.634034872055054 seconds, Average Loss: 6.744625965754191\n",
      "Epoch 101 finished in 63.82653498649597 seconds, Average Loss: 6.743606507778168\n",
      "Epoch 102 finished in 63.946202993392944 seconds, Average Loss: 6.741736610730489\n",
      "Epoch 103 finished in 64.3423798084259 seconds, Average Loss: 6.742920994758606\n",
      "Epoch 104 finished in 63.756523847579956 seconds, Average Loss: 6.7425229748090105\n",
      "Epoch 105 finished in 63.91386890411377 seconds, Average Loss: 6.741665760676066\n",
      "Epoch 106 finished in 63.74115490913391 seconds, Average Loss: 6.740264376004537\n",
      "Epoch 107 finished in 63.72882795333862 seconds, Average Loss: 6.7417044043540955\n",
      "Epoch 108 finished in 64.03781700134277 seconds, Average Loss: 6.737623512744904\n",
      "Epoch 109 finished in 63.72173309326172 seconds, Average Loss: 6.735326687494914\n",
      "Epoch 110 finished in 63.84492588043213 seconds, Average Loss: 6.737190226713817\n",
      "Epoch 111 finished in 63.814059019088745 seconds, Average Loss: 6.73795332511266\n",
      "Epoch 112 finished in 63.85216403007507 seconds, Average Loss: 6.737208843231201\n",
      "Epoch 113 finished in 63.92399311065674 seconds, Average Loss: 6.730297346909841\n",
      "Epoch 114 finished in 64.18499612808228 seconds, Average Loss: 6.7349178592364\n",
      "Epoch 115 finished in 63.647891998291016 seconds, Average Loss: 6.733210364977519\n",
      "Epoch 116 finished in 63.758432149887085 seconds, Average Loss: 6.730349044005076\n",
      "Epoch 117 finished in 63.811607122421265 seconds, Average Loss: 6.7297892570495605\n",
      "Epoch 118 finished in 63.82402515411377 seconds, Average Loss: 6.728260139624278\n",
      "Epoch 119 finished in 63.69462585449219 seconds, Average Loss: 6.727717439333598\n",
      "Epoch 120 finished in 63.64744997024536 seconds, Average Loss: 6.728976686795552\n",
      "Epoch 121 finished in 63.767098903656006 seconds, Average Loss: 6.727110127607982\n",
      "Epoch 122 finished in 63.75097584724426 seconds, Average Loss: 6.730975846449534\n",
      "Epoch 123 finished in 63.63617396354675 seconds, Average Loss: 6.726267655690511\n",
      "Epoch 124 finished in 63.8259642124176 seconds, Average Loss: 6.727377831935883\n",
      "Epoch 125 finished in 63.8337287902832 seconds, Average Loss: 6.726420124371846\n",
      "Epoch 126 finished in 63.6988959312439 seconds, Average Loss: 6.726792573928833\n",
      "Epoch 127 finished in 63.933542013168335 seconds, Average Loss: 6.725041131178538\n",
      "Epoch 128 finished in 63.94706201553345 seconds, Average Loss: 6.723684628804524\n",
      "Epoch 129 finished in 64.16843891143799 seconds, Average Loss: 6.720686097939809\n",
      "Epoch 130 finished in 63.92247223854065 seconds, Average Loss: 6.7237856189409895\n",
      "Epoch 131 finished in 63.78264498710632 seconds, Average Loss: 6.72086634238561\n",
      "Epoch 132 finished in 63.628618001937866 seconds, Average Loss: 6.719064474105835\n",
      "Epoch 133 finished in 63.84390187263489 seconds, Average Loss: 6.719256083170573\n",
      "Epoch 134 finished in 64.2547378540039 seconds, Average Loss: 6.721326271692912\n",
      "Epoch 135 finished in 63.81302499771118 seconds, Average Loss: 6.719937821229299\n",
      "Epoch 136 finished in 64.03947710990906 seconds, Average Loss: 6.722562750180562\n",
      "Epoch 137 finished in 64.32181978225708 seconds, Average Loss: 6.718644618988037\n",
      "Epoch 138 finished in 64.10915684700012 seconds, Average Loss: 6.718647499879201\n",
      "Epoch 139 finished in 63.76108908653259 seconds, Average Loss: 6.718664983908336\n",
      "Epoch 140 finished in 63.848737716674805 seconds, Average Loss: 6.718037664890289\n",
      "Epoch 141 finished in 63.62510013580322 seconds, Average Loss: 6.715481162071228\n",
      "Epoch 142 finished in 64.09265899658203 seconds, Average Loss: 6.714661101500194\n",
      "Epoch 143 finished in 64.08568811416626 seconds, Average Loss: 6.7157663106918335\n",
      "Epoch 144 finished in 63.90759086608887 seconds, Average Loss: 6.712874909241994\n",
      "Epoch 145 finished in 64.17487096786499 seconds, Average Loss: 6.710196773211162\n",
      "Epoch 146 finished in 64.01059293746948 seconds, Average Loss: 6.712645888328552\n",
      "Epoch 147 finished in 64.00662207603455 seconds, Average Loss: 6.7131123542785645\n",
      "Epoch 148 finished in 64.20945882797241 seconds, Average Loss: 6.713342189788818\n",
      "Epoch 149 finished in 63.881945848464966 seconds, Average Loss: 6.710040072600047\n",
      "Epoch 150 finished in 63.91460990905762 seconds, Average Loss: 6.7126186688741045\n",
      "Epoch 151 finished in 64.01318097114563 seconds, Average Loss: 6.711880445480347\n",
      "Epoch 152 finished in 63.83380699157715 seconds, Average Loss: 6.710921963055928\n",
      "Epoch 153 finished in 64.10475516319275 seconds, Average Loss: 6.711843887964885\n",
      "Epoch 154 finished in 64.04029297828674 seconds, Average Loss: 6.708524286746979\n",
      "Epoch 155 finished in 64.0509250164032 seconds, Average Loss: 6.709348817666371\n",
      "Epoch 156 finished in 64.30215787887573 seconds, Average Loss: 6.712675015131633\n",
      "Epoch 157 finished in 64.20532989501953 seconds, Average Loss: 6.710263053576152\n",
      "Epoch 158 finished in 64.13881182670593 seconds, Average Loss: 6.706874291102092\n",
      "Epoch 159 finished in 64.53451585769653 seconds, Average Loss: 6.709767758846283\n",
      "Epoch 160 finished in 64.06552386283875 seconds, Average Loss: 6.708817621072133\n",
      "Epoch 161 finished in 63.8589129447937 seconds, Average Loss: 6.705392122268677\n",
      "Epoch 162 finished in 64.0283682346344 seconds, Average Loss: 6.706687529881795\n",
      "Epoch 163 finished in 63.87831497192383 seconds, Average Loss: 6.7048555215199785\n",
      "Epoch 164 finished in 64.06402206420898 seconds, Average Loss: 6.706550697485606\n",
      "Epoch 165 finished in 63.92078709602356 seconds, Average Loss: 6.705699801445007\n",
      "Epoch 166 finished in 64.65850114822388 seconds, Average Loss: 6.7073396643002825\n",
      "Epoch 167 finished in 64.00944924354553 seconds, Average Loss: 6.705114940802257\n",
      "Epoch 168 finished in 64.16838884353638 seconds, Average Loss: 6.7054800391197205\n",
      "Epoch 169 finished in 64.2071361541748 seconds, Average Loss: 6.705524265766144\n",
      "Epoch 170 finished in 64.11370801925659 seconds, Average Loss: 6.700838327407837\n",
      "Epoch 171 finished in 63.87009310722351 seconds, Average Loss: 6.705665806929271\n",
      "Epoch 172 finished in 64.06660223007202 seconds, Average Loss: 6.706279794375102\n",
      "Epoch 173 finished in 63.924495220184326 seconds, Average Loss: 6.7042348980903625\n",
      "Epoch 174 finished in 63.78974485397339 seconds, Average Loss: 6.706366459528605\n",
      "Epoch 175 finished in 63.99277114868164 seconds, Average Loss: 6.702596624692281\n",
      "Epoch 176 finished in 63.80663013458252 seconds, Average Loss: 6.701052129268646\n",
      "Epoch 177 finished in 63.9347288608551 seconds, Average Loss: 6.705030679702759\n",
      "Epoch 178 finished in 64.17106199264526 seconds, Average Loss: 6.701602637767792\n",
      "Epoch 179 finished in 63.74506211280823 seconds, Average Loss: 6.703828990459442\n",
      "Epoch 180 finished in 63.99253225326538 seconds, Average Loss: 6.702560563882192\n",
      "Epoch 181 finished in 63.79064607620239 seconds, Average Loss: 6.701023876667023\n",
      "Epoch 182 finished in 64.22799277305603 seconds, Average Loss: 6.702512542406718\n",
      "Epoch 183 finished in 63.84555792808533 seconds, Average Loss: 6.702452798684438\n",
      "Epoch 184 finished in 64.00484204292297 seconds, Average Loss: 6.7023981014887495\n",
      "Epoch 185 finished in 64.02298378944397 seconds, Average Loss: 6.70399808883667\n",
      "Epoch 186 finished in 63.899001121520996 seconds, Average Loss: 6.704328219095866\n",
      "Epoch 187 finished in 63.9100558757782 seconds, Average Loss: 6.702478190263112\n",
      "Epoch 188 finished in 63.971158027648926 seconds, Average Loss: 6.702699999014537\n",
      "Epoch 189 finished in 63.906991958618164 seconds, Average Loss: 6.7005560000737505\n",
      "Epoch 190 finished in 64.00452876091003 seconds, Average Loss: 6.701600710550944\n",
      "Epoch 191 finished in 64.10456085205078 seconds, Average Loss: 6.699050664901733\n",
      "Epoch 192 finished in 63.91998791694641 seconds, Average Loss: 6.700965642929077\n",
      "Epoch 193 finished in 64.00365400314331 seconds, Average Loss: 6.7023493846257525\n",
      "Epoch 194 finished in 64.01883482933044 seconds, Average Loss: 6.7024819652239485\n",
      "Epoch 195 finished in 64.06900119781494 seconds, Average Loss: 6.7007894317309065\n",
      "Epoch 196 finished in 63.92210006713867 seconds, Average Loss: 6.701470176378886\n",
      "Epoch 197 finished in 63.93369197845459 seconds, Average Loss: 6.703135708967845\n",
      "Epoch 198 finished in 64.56589913368225 seconds, Average Loss: 6.704465568065643\n",
      "Epoch 199 finished in 64.32737302780151 seconds, Average Loss: 6.702246725559235\n",
      "Epoch 200 finished in 63.90099811553955 seconds, Average Loss: 6.7009527285893755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f875e23e0b74280abd86655eba720ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.70095</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-violet-16</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/8l3ikohf' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/8l3ikohf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_223804-8l3ikohf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240327_021658-ysd1p0kh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ysd1p0kh' target=\"_blank\">silver-cherry-17</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ysd1p0kh' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ysd1p0kh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 63.06705904006958 seconds, Average Loss: 7.652851541837056\n",
      "Epoch 2 finished in 61.95271587371826 seconds, Average Loss: 7.124851047992706\n",
      "Epoch 3 finished in 61.73205900192261 seconds, Average Loss: 6.891030251979828\n",
      "Epoch 4 finished in 61.99229407310486 seconds, Average Loss: 6.707362393538157\n",
      "Epoch 5 finished in 61.63967990875244 seconds, Average Loss: 6.604682425657908\n",
      "Epoch 6 finished in 61.689085960388184 seconds, Average Loss: 6.549993336200714\n",
      "Epoch 7 finished in 62.120986223220825 seconds, Average Loss: 6.517454703648885\n",
      "Epoch 8 finished in 61.614951848983765 seconds, Average Loss: 6.495660146077474\n",
      "Epoch 9 finished in 62.872801065444946 seconds, Average Loss: 6.478471120198567\n",
      "Epoch 10 finished in 61.74997305870056 seconds, Average Loss: 6.464324057102203\n",
      "Epoch 11 finished in 61.69476890563965 seconds, Average Loss: 6.453449110190074\n",
      "Epoch 12 finished in 62.56964087486267 seconds, Average Loss: 6.44720321893692\n",
      "Epoch 13 finished in 62.75314807891846 seconds, Average Loss: 6.439914385477702\n",
      "Epoch 14 finished in 61.86194705963135 seconds, Average Loss: 6.435259262720744\n",
      "Epoch 15 finished in 62.60786509513855 seconds, Average Loss: 6.431697289148967\n",
      "Epoch 16 finished in 62.394819021224976 seconds, Average Loss: 6.429196417331696\n",
      "Epoch 17 finished in 62.20616602897644 seconds, Average Loss: 6.426208198070526\n",
      "Epoch 18 finished in 61.875373125076294 seconds, Average Loss: 6.42414516210556\n",
      "Epoch 19 finished in 61.6851110458374 seconds, Average Loss: 6.422641416390737\n",
      "Epoch 20 finished in 61.87488102912903 seconds, Average Loss: 6.420207341512044\n",
      "Epoch 21 finished in 61.701974868774414 seconds, Average Loss: 6.4185777703921\n",
      "Epoch 22 finished in 61.66950488090515 seconds, Average Loss: 6.417500972747803\n",
      "Epoch 23 finished in 61.972066164016724 seconds, Average Loss: 6.41659688949585\n",
      "Epoch 24 finished in 61.69854784011841 seconds, Average Loss: 6.415764133135478\n",
      "Epoch 25 finished in 61.91075611114502 seconds, Average Loss: 6.4145440856615705\n",
      "Epoch 26 finished in 62.03080201148987 seconds, Average Loss: 6.4131313761075335\n",
      "Epoch 27 finished in 61.97324204444885 seconds, Average Loss: 6.41300642490387\n",
      "Epoch 28 finished in 62.04264998435974 seconds, Average Loss: 6.412894209225972\n",
      "Epoch 29 finished in 61.73305892944336 seconds, Average Loss: 6.411846439043681\n",
      "Epoch 30 finished in 61.61776804924011 seconds, Average Loss: 6.411064922809601\n",
      "Epoch 31 finished in 61.63139510154724 seconds, Average Loss: 6.41080904006958\n",
      "Epoch 32 finished in 61.74181294441223 seconds, Average Loss: 6.4100136160850525\n",
      "Epoch 33 finished in 61.618041038513184 seconds, Average Loss: 6.409490764141083\n",
      "Epoch 34 finished in 61.62159705162048 seconds, Average Loss: 6.409412344296773\n",
      "Epoch 35 finished in 61.677488803863525 seconds, Average Loss: 6.408630828062694\n",
      "Epoch 36 finished in 61.72334098815918 seconds, Average Loss: 6.408605635166168\n",
      "Epoch 37 finished in 61.794878005981445 seconds, Average Loss: 6.408440013726552\n",
      "Epoch 38 finished in 61.71961498260498 seconds, Average Loss: 6.407771269480388\n",
      "Epoch 39 finished in 61.80812311172485 seconds, Average Loss: 6.407738884290059\n",
      "Epoch 40 finished in 61.66565704345703 seconds, Average Loss: 6.407624125480652\n",
      "Epoch 41 finished in 61.71574902534485 seconds, Average Loss: 6.407438218593597\n",
      "Epoch 42 finished in 62.146323919296265 seconds, Average Loss: 6.406580785910289\n",
      "Epoch 43 finished in 62.299219846725464 seconds, Average Loss: 6.406079709529877\n",
      "Epoch 44 finished in 61.85391402244568 seconds, Average Loss: 6.405655145645142\n",
      "Epoch 45 finished in 62.27643799781799 seconds, Average Loss: 6.405780831972758\n",
      "Epoch 46 finished in 61.83082318305969 seconds, Average Loss: 6.405252397060394\n",
      "Epoch 47 finished in 62.14042401313782 seconds, Average Loss: 6.4055490891138716\n",
      "Epoch 48 finished in 61.827608823776245 seconds, Average Loss: 6.405267655849457\n",
      "Epoch 49 finished in 61.898432970047 seconds, Average Loss: 6.404667913913727\n",
      "Epoch 50 finished in 61.9888870716095 seconds, Average Loss: 6.4048664172490435\n",
      "Epoch 51 finished in 61.688997745513916 seconds, Average Loss: 6.404460430145264\n",
      "Epoch 52 finished in 61.93997883796692 seconds, Average Loss: 6.404537121454875\n",
      "Epoch 53 finished in 61.75878286361694 seconds, Average Loss: 6.4038077394167585\n",
      "Epoch 54 finished in 61.73594307899475 seconds, Average Loss: 6.4039152065912885\n",
      "Epoch 55 finished in 62.25362205505371 seconds, Average Loss: 6.40348494052887\n",
      "Epoch 56 finished in 62.06252908706665 seconds, Average Loss: 6.403131087621053\n",
      "Epoch 57 finished in 62.268754959106445 seconds, Average Loss: 6.402903497219086\n",
      "Epoch 58 finished in 61.929949045181274 seconds, Average Loss: 6.403205434481303\n",
      "Epoch 59 finished in 61.822295904159546 seconds, Average Loss: 6.402818560600281\n",
      "Epoch 60 finished in 62.048019886016846 seconds, Average Loss: 6.402949869632721\n",
      "Epoch 61 finished in 62.06007218360901 seconds, Average Loss: 6.402285774548848\n",
      "Epoch 62 finished in 61.65860390663147 seconds, Average Loss: 6.402241508165996\n",
      "Epoch 63 finished in 61.69753384590149 seconds, Average Loss: 6.401890416940053\n",
      "Epoch 64 finished in 61.85293507575989 seconds, Average Loss: 6.402233521143596\n",
      "Epoch 65 finished in 61.96321988105774 seconds, Average Loss: 6.40153553088506\n",
      "Epoch 66 finished in 61.71233105659485 seconds, Average Loss: 6.401499728361766\n",
      "Epoch 67 finished in 61.90136194229126 seconds, Average Loss: 6.4014306863149\n",
      "Epoch 68 finished in 61.87605810165405 seconds, Average Loss: 6.4012075662612915\n",
      "Epoch 69 finished in 61.81841993331909 seconds, Average Loss: 6.401023507118225\n",
      "Epoch 70 finished in 61.9342520236969 seconds, Average Loss: 6.400983989238739\n",
      "Epoch 71 finished in 62.591450214385986 seconds, Average Loss: 6.400408764680226\n",
      "Epoch 72 finished in 62.061683893203735 seconds, Average Loss: 6.400653302669525\n",
      "Epoch 73 finished in 62.31463289260864 seconds, Average Loss: 6.400295456250508\n",
      "Epoch 74 finished in 62.368828773498535 seconds, Average Loss: 6.400204678376515\n",
      "Epoch 75 finished in 61.74878191947937 seconds, Average Loss: 6.399845699469249\n",
      "Epoch 76 finished in 61.91422367095947 seconds, Average Loss: 6.399615406990051\n",
      "Epoch 77 finished in 61.84417390823364 seconds, Average Loss: 6.399946928024292\n",
      "Epoch 78 finished in 61.770957708358765 seconds, Average Loss: 6.398895641167958\n",
      "Epoch 79 finished in 61.72320318222046 seconds, Average Loss: 6.399281322956085\n",
      "Epoch 80 finished in 61.81006217002869 seconds, Average Loss: 6.39899617433548\n",
      "Epoch 81 finished in 61.855854988098145 seconds, Average Loss: 6.398535867532094\n",
      "Epoch 82 finished in 61.60793924331665 seconds, Average Loss: 6.3983726898829145\n",
      "Epoch 83 finished in 61.56898522377014 seconds, Average Loss: 6.3986327449480696\n",
      "Epoch 84 finished in 61.87934589385986 seconds, Average Loss: 6.398759722709656\n",
      "Epoch 85 finished in 61.95251107215881 seconds, Average Loss: 6.398127098878224\n",
      "Epoch 86 finished in 62.92611002922058 seconds, Average Loss: 6.39832862218221\n",
      "Epoch 87 finished in 62.29386901855469 seconds, Average Loss: 6.397764285405477\n",
      "Epoch 88 finished in 61.67806959152222 seconds, Average Loss: 6.397587219874064\n",
      "Epoch 89 finished in 61.499115228652954 seconds, Average Loss: 6.397345046202342\n",
      "Epoch 90 finished in 61.610875844955444 seconds, Average Loss: 6.397139728069305\n",
      "Epoch 91 finished in 61.84548902511597 seconds, Average Loss: 6.397242188453674\n",
      "Epoch 92 finished in 61.6632661819458 seconds, Average Loss: 6.397081871827443\n",
      "Epoch 93 finished in 61.83245325088501 seconds, Average Loss: 6.396902044614156\n",
      "Epoch 94 finished in 61.677046060562134 seconds, Average Loss: 6.3969990611076355\n",
      "Epoch 95 finished in 62.04967021942139 seconds, Average Loss: 6.396810213724772\n",
      "Epoch 96 finished in 61.708423137664795 seconds, Average Loss: 6.3967918157577515\n",
      "Epoch 97 finished in 61.88822102546692 seconds, Average Loss: 6.396202047665914\n",
      "Epoch 98 finished in 61.83505392074585 seconds, Average Loss: 6.396284182866414\n",
      "Epoch 99 finished in 61.910881996154785 seconds, Average Loss: 6.3960622151692705\n",
      "Epoch 100 finished in 62.971872091293335 seconds, Average Loss: 6.395794888337453\n",
      "Epoch 101 finished in 61.658539056777954 seconds, Average Loss: 6.395411590735118\n",
      "Epoch 102 finished in 62.136656761169434 seconds, Average Loss: 6.395803511142731\n",
      "Epoch 103 finished in 61.878973960876465 seconds, Average Loss: 6.395277400811513\n",
      "Epoch 104 finished in 61.611905097961426 seconds, Average Loss: 6.395678063233693\n",
      "Epoch 105 finished in 61.66189908981323 seconds, Average Loss: 6.3953343232472735\n",
      "Epoch 106 finished in 62.001641035079956 seconds, Average Loss: 6.394849181175232\n",
      "Epoch 107 finished in 61.94186806678772 seconds, Average Loss: 6.394728501637776\n",
      "Epoch 108 finished in 61.83004379272461 seconds, Average Loss: 6.394561052322388\n",
      "Epoch 109 finished in 61.94948673248291 seconds, Average Loss: 6.394619782765706\n",
      "Epoch 110 finished in 61.861867904663086 seconds, Average Loss: 6.394348382949829\n",
      "Epoch 111 finished in 61.846598863601685 seconds, Average Loss: 6.394085089365642\n",
      "Epoch 112 finished in 61.869584798812866 seconds, Average Loss: 6.394081970055898\n",
      "Epoch 113 finished in 61.74340772628784 seconds, Average Loss: 6.393884698549907\n",
      "Epoch 114 finished in 62.265015840530396 seconds, Average Loss: 6.393648783365886\n",
      "Epoch 115 finished in 62.2005980014801 seconds, Average Loss: 6.393581926822662\n",
      "Epoch 116 finished in 62.19808578491211 seconds, Average Loss: 6.39326936006546\n",
      "Epoch 117 finished in 61.735647678375244 seconds, Average Loss: 6.393411338329315\n",
      "Epoch 118 finished in 62.16938328742981 seconds, Average Loss: 6.393490135669708\n",
      "Epoch 119 finished in 61.881852865219116 seconds, Average Loss: 6.3931659658749895\n",
      "Epoch 120 finished in 62.089096784591675 seconds, Average Loss: 6.392555276552836\n",
      "Epoch 121 finished in 61.98871207237244 seconds, Average Loss: 6.392573952674866\n",
      "Epoch 122 finished in 61.85657095909119 seconds, Average Loss: 6.3923167785008745\n",
      "Epoch 123 finished in 61.835354804992676 seconds, Average Loss: 6.392292936642964\n",
      "Epoch 124 finished in 61.931718826293945 seconds, Average Loss: 6.392441689968109\n",
      "Epoch 125 finished in 62.40617489814758 seconds, Average Loss: 6.391884942849477\n",
      "Epoch 126 finished in 61.752135276794434 seconds, Average Loss: 6.391848981380463\n",
      "Epoch 127 finished in 62.00291919708252 seconds, Average Loss: 6.391690452893575\n",
      "Epoch 128 finished in 62.045539140701294 seconds, Average Loss: 6.391585131486257\n",
      "Epoch 129 finished in 62.08655023574829 seconds, Average Loss: 6.391628702481587\n",
      "Epoch 130 finished in 61.7418577671051 seconds, Average Loss: 6.391192734241486\n",
      "Epoch 131 finished in 61.8439679145813 seconds, Average Loss: 6.3909123341242475\n",
      "Epoch 132 finished in 62.727550983428955 seconds, Average Loss: 6.391001999378204\n",
      "Epoch 133 finished in 62.09910607337952 seconds, Average Loss: 6.390512625376384\n",
      "Epoch 134 finished in 61.819758892059326 seconds, Average Loss: 6.3902738491694135\n",
      "Epoch 135 finished in 62.38140106201172 seconds, Average Loss: 6.390573024749756\n",
      "Epoch 136 finished in 61.77325105667114 seconds, Average Loss: 6.390261967976888\n",
      "Epoch 137 finished in 61.80965709686279 seconds, Average Loss: 6.390046199162801\n",
      "Epoch 138 finished in 62.336960792541504 seconds, Average Loss: 6.3900692264239\n",
      "Epoch 139 finished in 62.015913009643555 seconds, Average Loss: 6.390150586764018\n",
      "Epoch 140 finished in 61.801326751708984 seconds, Average Loss: 6.389678120613098\n",
      "Epoch 141 finished in 62.55949306488037 seconds, Average Loss: 6.3897389968236284\n",
      "Epoch 142 finished in 62.175339221954346 seconds, Average Loss: 6.3892889221509295\n",
      "Epoch 143 finished in 62.2912700176239 seconds, Average Loss: 6.389187157154083\n",
      "Epoch 144 finished in 62.38516879081726 seconds, Average Loss: 6.389220774173737\n",
      "Epoch 145 finished in 62.259641885757446 seconds, Average Loss: 6.389049828052521\n",
      "Epoch 146 finished in 61.83051681518555 seconds, Average Loss: 6.388642688592275\n",
      "Epoch 147 finished in 61.97376990318298 seconds, Average Loss: 6.388742089271545\n",
      "Epoch 148 finished in 61.959264039993286 seconds, Average Loss: 6.388563175996144\n",
      "Epoch 149 finished in 61.62939095497131 seconds, Average Loss: 6.388469974199931\n",
      "Epoch 150 finished in 61.727388858795166 seconds, Average Loss: 6.388300855954488\n",
      "Epoch 151 finished in 61.77286720275879 seconds, Average Loss: 6.388356626033783\n",
      "Epoch 152 finished in 61.87607502937317 seconds, Average Loss: 6.388285378615062\n",
      "Epoch 153 finished in 62.04203391075134 seconds, Average Loss: 6.388055324554443\n",
      "Epoch 154 finished in 62.1879940032959 seconds, Average Loss: 6.387867311636607\n",
      "Epoch 155 finished in 61.77408695220947 seconds, Average Loss: 6.387845754623413\n",
      "Epoch 156 finished in 62.0331449508667 seconds, Average Loss: 6.387763539950053\n",
      "Epoch 157 finished in 62.07467293739319 seconds, Average Loss: 6.387486159801483\n",
      "Epoch 158 finished in 62.36139678955078 seconds, Average Loss: 6.387651880582173\n",
      "Epoch 159 finished in 61.84550404548645 seconds, Average Loss: 6.387511432170868\n",
      "Epoch 160 finished in 62.36125707626343 seconds, Average Loss: 6.387326260407765\n",
      "Epoch 161 finished in 62.13297390937805 seconds, Average Loss: 6.386978308359782\n",
      "Epoch 162 finished in 62.06854224205017 seconds, Average Loss: 6.386873265107472\n",
      "Epoch 163 finished in 62.50614929199219 seconds, Average Loss: 6.38708883523941\n",
      "Epoch 164 finished in 62.63536787033081 seconds, Average Loss: 6.38689402739207\n",
      "Epoch 165 finished in 61.86259603500366 seconds, Average Loss: 6.386565426985423\n",
      "Epoch 166 finished in 62.08210301399231 seconds, Average Loss: 6.386513392130534\n",
      "Epoch 167 finished in 62.29903697967529 seconds, Average Loss: 6.386358380317688\n",
      "Epoch 168 finished in 61.64700484275818 seconds, Average Loss: 6.386234104633331\n",
      "Epoch 169 finished in 62.04931712150574 seconds, Average Loss: 6.386036892731984\n",
      "Epoch 170 finished in 62.10608196258545 seconds, Average Loss: 6.386557261149089\n",
      "Epoch 171 finished in 61.99203109741211 seconds, Average Loss: 6.386384665966034\n",
      "Epoch 172 finished in 61.933034896850586 seconds, Average Loss: 6.386394580205281\n",
      "Epoch 173 finished in 62.39649701118469 seconds, Average Loss: 6.385838627815247\n",
      "Epoch 174 finished in 62.14584803581238 seconds, Average Loss: 6.386263132095337\n",
      "Epoch 175 finished in 62.29642415046692 seconds, Average Loss: 6.385919610659282\n",
      "Epoch 176 finished in 62.29266381263733 seconds, Average Loss: 6.385926167170207\n",
      "Epoch 177 finished in 65.30306100845337 seconds, Average Loss: 6.3859818776448565\n",
      "Epoch 178 finished in 61.70750689506531 seconds, Average Loss: 6.385842581590016\n",
      "Epoch 179 finished in 61.74989891052246 seconds, Average Loss: 6.385864794254303\n",
      "Epoch 180 finished in 61.863529205322266 seconds, Average Loss: 6.385773777961731\n",
      "Epoch 181 finished in 62.06131100654602 seconds, Average Loss: 6.3856252034505205\n",
      "Epoch 182 finished in 62.1064567565918 seconds, Average Loss: 6.3856703241666155\n",
      "Epoch 183 finished in 62.645256757736206 seconds, Average Loss: 6.385662833849589\n",
      "Epoch 184 finished in 61.85913896560669 seconds, Average Loss: 6.385770916938782\n",
      "Epoch 185 finished in 62.04193902015686 seconds, Average Loss: 6.385533809661865\n",
      "Epoch 186 finished in 61.87252712249756 seconds, Average Loss: 6.385446051756541\n",
      "Epoch 187 finished in 61.74444270133972 seconds, Average Loss: 6.385771572589874\n",
      "Epoch 188 finished in 61.87307381629944 seconds, Average Loss: 6.385448018709819\n",
      "Epoch 189 finished in 62.149760723114014 seconds, Average Loss: 6.385675986607869\n",
      "Epoch 190 finished in 62.52705383300781 seconds, Average Loss: 6.385301887989044\n",
      "Epoch 191 finished in 61.76517415046692 seconds, Average Loss: 6.38547021150589\n",
      "Epoch 192 finished in 62.716150999069214 seconds, Average Loss: 6.385301013787587\n",
      "Epoch 193 finished in 61.804839849472046 seconds, Average Loss: 6.385357975959778\n",
      "Epoch 194 finished in 61.77564287185669 seconds, Average Loss: 6.385084370772044\n",
      "Epoch 195 finished in 61.795904874801636 seconds, Average Loss: 6.385293543338776\n",
      "Epoch 196 finished in 62.13086414337158 seconds, Average Loss: 6.385442137718201\n",
      "Epoch 197 finished in 62.16020202636719 seconds, Average Loss: 6.385577102502187\n",
      "Epoch 198 finished in 62.13050317764282 seconds, Average Loss: 6.385355273882548\n",
      "Epoch 199 finished in 61.997262716293335 seconds, Average Loss: 6.385392884413402\n",
      "Epoch 200 finished in 62.76298379898071 seconds, Average Loss: 6.38530158996582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbd3005c23c44e2bcc2b82ada87679f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.3853</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-cherry-17</strong> at: <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ysd1p0kh' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/ysd1p0kh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240327_021658-ysd1p0kh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logs saved.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/t4krq7213hgccsq3l82cs_900000gp/T/ipykernel_59179/1349866135.py:40: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stevan.matovic@onetrust.com/Studies/papers/SimCLR-tiny-imagenet/wandb/run-20240327_054347-09lx8wt2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/09lx8wt2' target=\"_blank\">proud-voice-18</a></strong> to <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/09lx8wt2' target=\"_blank\">https://wandb.ai/stevan-matovic/SimCLR-augmentation-experiments/runs/09lx8wt2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished in 84.73026871681213 seconds, Average Loss: 7.719161589940389\n",
      "Epoch 2 finished in 86.92192268371582 seconds, Average Loss: 7.134414434432983\n",
      "Epoch 3 finished in 84.03852391242981 seconds, Average Loss: 6.843050042788188\n",
      "Epoch 4 finished in 83.27088570594788 seconds, Average Loss: 6.725333412488301\n",
      "Epoch 5 finished in 84.79025411605835 seconds, Average Loss: 6.632374405860901\n",
      "Epoch 6 finished in 84.05186104774475 seconds, Average Loss: 6.561048169930776\n",
      "Epoch 7 finished in 83.91335487365723 seconds, Average Loss: 6.5234248240788775\n",
      "Epoch 8 finished in 84.04877209663391 seconds, Average Loss: 6.501716951529185\n",
      "Epoch 9 finished in 84.18213820457458 seconds, Average Loss: 6.484394033749898\n",
      "Epoch 10 finished in 84.07227921485901 seconds, Average Loss: 6.47136648495992\n",
      "Epoch 11 finished in 84.67604088783264 seconds, Average Loss: 6.460646351178487\n",
      "Epoch 12 finished in 85.01595902442932 seconds, Average Loss: 6.451785524686177\n",
      "Epoch 13 finished in 85.30078530311584 seconds, Average Loss: 6.445229132970174\n",
      "Epoch 14 finished in 85.12518215179443 seconds, Average Loss: 6.440116127332051\n",
      "Epoch 15 finished in 85.28171110153198 seconds, Average Loss: 6.437044282754262\n",
      "Epoch 16 finished in 83.95282006263733 seconds, Average Loss: 6.432746907075246\n",
      "Epoch 17 finished in 84.60029816627502 seconds, Average Loss: 6.429459551970164\n",
      "Epoch 18 finished in 85.69310402870178 seconds, Average Loss: 6.427071372667949\n",
      "Epoch 19 finished in 84.59942102432251 seconds, Average Loss: 6.424720327059428\n",
      "Epoch 20 finished in 85.01639795303345 seconds, Average Loss: 6.423238635063171\n",
      "Epoch 21 finished in 84.31010794639587 seconds, Average Loss: 6.4218728343645735\n",
      "Epoch 22 finished in 88.14565396308899 seconds, Average Loss: 6.420235713322957\n",
      "Epoch 23 finished in 86.72124195098877 seconds, Average Loss: 6.419696827729543\n",
      "Epoch 24 finished in 85.71948313713074 seconds, Average Loss: 6.418251534303029\n",
      "Epoch 25 finished in 83.87396788597107 seconds, Average Loss: 6.417080879211426\n",
      "Epoch 26 finished in 84.54775500297546 seconds, Average Loss: 6.415627539157867\n",
      "Epoch 27 finished in 84.46710920333862 seconds, Average Loss: 6.415333131949107\n",
      "Epoch 28 finished in 85.3430290222168 seconds, Average Loss: 6.4150452216466265\n",
      "Epoch 29 finished in 84.67749881744385 seconds, Average Loss: 6.413712064425151\n",
      "Epoch 30 finished in 85.08605122566223 seconds, Average Loss: 6.41307810942332\n",
      "Epoch 31 finished in 85.39133310317993 seconds, Average Loss: 6.412746369838715\n",
      "Epoch 32 finished in 85.1149959564209 seconds, Average Loss: 6.4120553731918335\n",
      "Epoch 33 finished in 84.89135599136353 seconds, Average Loss: 6.411165912946065\n",
      "Epoch 34 finished in 85.96324801445007 seconds, Average Loss: 6.410860498746236\n",
      "Epoch 35 finished in 86.05965805053711 seconds, Average Loss: 6.411123792330424\n",
      "Epoch 36 finished in 86.99055123329163 seconds, Average Loss: 6.410734335581462\n",
      "Epoch 37 finished in 86.22225308418274 seconds, Average Loss: 6.410415907700856\n",
      "Epoch 38 finished in 85.27991199493408 seconds, Average Loss: 6.4098387360572815\n",
      "Epoch 39 finished in 84.93100500106812 seconds, Average Loss: 6.408903161684672\n",
      "Epoch 40 finished in 86.1229727268219 seconds, Average Loss: 6.408674538135529\n",
      "Epoch 41 finished in 86.4584093093872 seconds, Average Loss: 6.409095446268718\n",
      "Epoch 42 finished in 85.3730149269104 seconds, Average Loss: 6.408357044061025\n",
      "Epoch 43 finished in 85.2818820476532 seconds, Average Loss: 6.407688697179158\n",
      "Epoch 44 finished in 86.03215432167053 seconds, Average Loss: 6.407664279143016\n",
      "Epoch 45 finished in 85.55145311355591 seconds, Average Loss: 6.407692829767863\n",
      "Epoch 46 finished in 86.65913772583008 seconds, Average Loss: 6.407638370990753\n",
      "Epoch 47 finished in 85.21020603179932 seconds, Average Loss: 6.407085796197255\n",
      "Epoch 48 finished in 85.18299412727356 seconds, Average Loss: 6.407102843125661\n",
      "Epoch 49 finished in 84.6875319480896 seconds, Average Loss: 6.406609574953715\n",
      "Epoch 50 finished in 83.77005076408386 seconds, Average Loss: 6.406576931476593\n",
      "Epoch 51 finished in 84.16466498374939 seconds, Average Loss: 6.40584667523702\n",
      "Epoch 52 finished in 84.8857159614563 seconds, Average Loss: 6.4060168862342834\n",
      "Epoch 53 finished in 84.31411814689636 seconds, Average Loss: 6.405365924040477\n",
      "Epoch 54 finished in 84.64585208892822 seconds, Average Loss: 6.405535638332367\n",
      "Epoch 55 finished in 85.01785898208618 seconds, Average Loss: 6.405221879482269\n",
      "Epoch 56 finished in 84.84773182868958 seconds, Average Loss: 6.405114849408467\n",
      "Epoch 57 finished in 84.2931010723114 seconds, Average Loss: 6.404741108417511\n",
      "Epoch 58 finished in 84.90680599212646 seconds, Average Loss: 6.404802322387695\n",
      "Epoch 59 finished in 84.91539287567139 seconds, Average Loss: 6.404371678829193\n",
      "Epoch 60 finished in 85.65568614006042 seconds, Average Loss: 6.404448390007019\n",
      "Epoch 61 finished in 85.68579506874084 seconds, Average Loss: 6.404393533865611\n",
      "Epoch 62 finished in 85.83762288093567 seconds, Average Loss: 6.4037982026735945\n",
      "Epoch 63 finished in 85.59798693656921 seconds, Average Loss: 6.4040489594141645\n",
      "Epoch 64 finished in 85.5688099861145 seconds, Average Loss: 6.4037390152613325\n",
      "Epoch 65 finished in 87.13248586654663 seconds, Average Loss: 6.403721928596497\n",
      "Epoch 66 finished in 85.3453381061554 seconds, Average Loss: 6.403849899768829\n",
      "Epoch 67 finished in 85.50462532043457 seconds, Average Loss: 6.402974526087443\n",
      "Epoch 68 finished in 85.08277988433838 seconds, Average Loss: 6.402764280637105\n",
      "Epoch 69 finished in 86.55640411376953 seconds, Average Loss: 6.402776340643565\n",
      "Epoch 70 finished in 86.31457996368408 seconds, Average Loss: 6.4024366935094195\n",
      "Epoch 71 finished in 86.33304405212402 seconds, Average Loss: 6.402957558631897\n",
      "Epoch 72 finished in 85.98901987075806 seconds, Average Loss: 6.402468800544739\n",
      "Epoch 73 finished in 85.79359102249146 seconds, Average Loss: 6.402431825796763\n",
      "Epoch 74 finished in 85.51104617118835 seconds, Average Loss: 6.4019331733385725\n",
      "Epoch 75 finished in 86.79288077354431 seconds, Average Loss: 6.401914099852244\n",
      "Epoch 76 finished in 85.721107006073 seconds, Average Loss: 6.401915808518727\n",
      "Epoch 77 finished in 87.87809205055237 seconds, Average Loss: 6.401260276635488\n",
      "Epoch 78 finished in 86.60458064079285 seconds, Average Loss: 6.4013460874557495\n",
      "Epoch 79 finished in 87.41350197792053 seconds, Average Loss: 6.401232937971751\n",
      "Epoch 80 finished in 86.03004789352417 seconds, Average Loss: 6.400792956352234\n",
      "Epoch 81 finished in 87.25465989112854 seconds, Average Loss: 6.40093857049942\n",
      "Epoch 82 finished in 88.3569700717926 seconds, Average Loss: 6.400671819845836\n",
      "Epoch 83 finished in 84.57456707954407 seconds, Average Loss: 6.40018888314565\n",
      "Epoch 84 finished in 85.24663805961609 seconds, Average Loss: 6.400792896747589\n",
      "Epoch 85 finished in 85.31561994552612 seconds, Average Loss: 6.400153398513794\n",
      "Epoch 86 finished in 88.30061507225037 seconds, Average Loss: 6.3998898069063825\n",
      "Epoch 87 finished in 86.36050295829773 seconds, Average Loss: 6.3999064564704895\n",
      "Epoch 88 finished in 84.49039196968079 seconds, Average Loss: 6.399659216403961\n",
      "Epoch 89 finished in 91.40984606742859 seconds, Average Loss: 6.39949357509613\n",
      "Epoch 90 finished in 84.0986340045929 seconds, Average Loss: 6.400144656499227\n",
      "Epoch 91 finished in 84.11311912536621 seconds, Average Loss: 6.399109264214833\n",
      "Epoch 92 finished in 84.01015591621399 seconds, Average Loss: 6.399215400218964\n",
      "Epoch 93 finished in 83.99280095100403 seconds, Average Loss: 6.399222036202748\n",
      "Epoch 94 finished in 84.18914079666138 seconds, Average Loss: 6.398632963498433\n",
      "Epoch 95 finished in 83.93956995010376 seconds, Average Loss: 6.398861626784007\n",
      "Epoch 96 finished in 84.25681185722351 seconds, Average Loss: 6.398614764213562\n",
      "Epoch 97 finished in 84.75605702400208 seconds, Average Loss: 6.398436148961385\n",
      "Epoch 98 finished in 84.91625905036926 seconds, Average Loss: 6.398074924945831\n",
      "Epoch 99 finished in 84.22555494308472 seconds, Average Loss: 6.39793445666631\n",
      "Epoch 100 finished in 84.87837886810303 seconds, Average Loss: 6.398588001728058\n",
      "Epoch 101 finished in 84.92586398124695 seconds, Average Loss: 6.398148934046428\n",
      "Epoch 102 finished in 84.98901200294495 seconds, Average Loss: 6.397707045078278\n",
      "Epoch 103 finished in 85.19335079193115 seconds, Average Loss: 6.397539913654327\n",
      "Epoch 104 finished in 86.18245124816895 seconds, Average Loss: 6.397428532441457\n",
      "Epoch 105 finished in 86.2815568447113 seconds, Average Loss: 6.397533198197682\n",
      "Epoch 106 finished in 86.02193999290466 seconds, Average Loss: 6.397042512893677\n",
      "Epoch 107 finished in 86.77430009841919 seconds, Average Loss: 6.396812816460927\n",
      "Epoch 108 finished in 85.53755593299866 seconds, Average Loss: 6.396718263626099\n",
      "Epoch 109 finished in 85.14346504211426 seconds, Average Loss: 6.396787544091542\n",
      "Epoch 110 finished in 84.77426886558533 seconds, Average Loss: 6.396607557932536\n",
      "Epoch 111 finished in 84.64878177642822 seconds, Average Loss: 6.39645258585612\n",
      "Epoch 112 finished in 85.25779724121094 seconds, Average Loss: 6.396367847919464\n",
      "Epoch 113 finished in 84.92761898040771 seconds, Average Loss: 6.396021127700806\n",
      "Epoch 114 finished in 84.33301591873169 seconds, Average Loss: 6.396196444829305\n",
      "Epoch 115 finished in 85.70340824127197 seconds, Average Loss: 6.395534793535869\n",
      "Epoch 116 finished in 84.76220893859863 seconds, Average Loss: 6.395693560441335\n",
      "Epoch 117 finished in 85.97235703468323 seconds, Average Loss: 6.395213166872661\n",
      "Epoch 118 finished in 85.11530423164368 seconds, Average Loss: 6.395305812358856\n",
      "Epoch 119 finished in 87.40198588371277 seconds, Average Loss: 6.3950245181719465\n",
      "Epoch 120 finished in 86.16811203956604 seconds, Average Loss: 6.394843320051829\n",
      "Epoch 121 finished in 85.40055298805237 seconds, Average Loss: 6.394850373268127\n",
      "Epoch 122 finished in 84.66624522209167 seconds, Average Loss: 6.395021518071492\n",
      "Epoch 123 finished in 86.05206799507141 seconds, Average Loss: 6.394447008768718\n",
      "Epoch 124 finished in 85.34214305877686 seconds, Average Loss: 6.394225398699443\n",
      "Epoch 125 finished in 85.00801301002502 seconds, Average Loss: 6.3945198853810625\n",
      "Epoch 126 finished in 85.52522611618042 seconds, Average Loss: 6.394224941730499\n",
      "Epoch 127 finished in 84.80550599098206 seconds, Average Loss: 6.394062956174214\n",
      "Epoch 128 finished in 84.79664492607117 seconds, Average Loss: 6.394073307514191\n",
      "Epoch 129 finished in 86.77127981185913 seconds, Average Loss: 6.3935200770696\n",
      "Epoch 130 finished in 85.11237573623657 seconds, Average Loss: 6.393403808275859\n",
      "Epoch 131 finished in 85.15463185310364 seconds, Average Loss: 6.393495082855225\n",
      "Epoch 132 finished in 84.15891695022583 seconds, Average Loss: 6.393524348735809\n",
      "Epoch 133 finished in 84.43097114562988 seconds, Average Loss: 6.393328309059143\n",
      "Epoch 134 finished in 84.97158193588257 seconds, Average Loss: 6.3931881586710615\n",
      "Epoch 135 finished in 84.97288513183594 seconds, Average Loss: 6.3925726016362505\n",
      "Epoch 136 finished in 85.46237707138062 seconds, Average Loss: 6.392654736836751\n",
      "Epoch 137 finished in 84.5445339679718 seconds, Average Loss: 6.392995158831279\n",
      "Epoch 138 finished in 84.94041609764099 seconds, Average Loss: 6.392306963602702\n",
      "Epoch 139 finished in 85.46383881568909 seconds, Average Loss: 6.392054438591003\n",
      "Epoch 140 finished in 85.26902985572815 seconds, Average Loss: 6.392153978347778\n",
      "Epoch 141 finished in 84.64053511619568 seconds, Average Loss: 6.3923812707265215\n",
      "Epoch 142 finished in 86.90843105316162 seconds, Average Loss: 6.3922185699145\n",
      "Epoch 143 finished in 85.30695295333862 seconds, Average Loss: 6.391873478889465\n",
      "Epoch 144 finished in 85.90161514282227 seconds, Average Loss: 6.391609450181325\n",
      "Epoch 145 finished in 85.91415786743164 seconds, Average Loss: 6.391252060731252\n",
      "Epoch 146 finished in 86.5439863204956 seconds, Average Loss: 6.391720652580261\n",
      "Epoch 147 finished in 86.11393094062805 seconds, Average Loss: 6.391875763734181\n",
      "Epoch 148 finished in 86.03883695602417 seconds, Average Loss: 6.3914985458056135\n",
      "Epoch 149 finished in 89.07481813430786 seconds, Average Loss: 6.391142229239146\n",
      "Epoch 150 finished in 86.54381799697876 seconds, Average Loss: 6.391302148501079\n",
      "Epoch 151 finished in 85.54376482963562 seconds, Average Loss: 6.390744825204213\n",
      "Epoch 152 finished in 85.61169195175171 seconds, Average Loss: 6.390985349814097\n",
      "Epoch 153 finished in 85.89133191108704 seconds, Average Loss: 6.390617867310842\n",
      "Epoch 154 finished in 84.90511298179626 seconds, Average Loss: 6.390750328699748\n",
      "Epoch 155 finished in 93.28302001953125 seconds, Average Loss: 6.39055722951889\n",
      "Epoch 156 finished in 83.39439487457275 seconds, Average Loss: 6.390314956506093\n",
      "Epoch 157 finished in 82.52963995933533 seconds, Average Loss: 6.390490889549255\n",
      "Epoch 158 finished in 82.8872447013855 seconds, Average Loss: 6.390480538209279\n",
      "Epoch 159 finished in 87.39935517311096 seconds, Average Loss: 6.390005985895793\n",
      "Epoch 160 finished in 97.56717586517334 seconds, Average Loss: 6.390067934989929\n",
      "Epoch 161 finished in 84.66310405731201 seconds, Average Loss: 6.389744818210602\n",
      "Epoch 162 finished in 90.38318204879761 seconds, Average Loss: 6.389572620391846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from data import get_color_distortion\n",
    "from model import SimCLR, NTXentLoss\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Training on\", device)\n",
    "\n",
    "# Define a pair transformation that applies the transformation only for one branch of the model\n",
    "class PairTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "        self.original_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.original_transform(x), self.transform(x)\n",
    "\n",
    "def run_experiment(config, transform):\n",
    "    if config[\"dataset\"] == \"CIFAR-10\":\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root='./data', transform=PairTransform(transform), download=True)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "\n",
    "    model = SimCLR(resnet=config[\"resnet\"], out_dim=config[\"projection_dim\"], projection=config[\"projection\"]).to(device)\n",
    "    criterion = NTXentLoss(config[\"batch_size\"], device, config[\"temperature\"])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=config[\"warmup_epochs\"], warmup_start_lr=config[\"learning_rate\"] * 1/10, max_epochs=config[\"epochs\"], eta_min=0)\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"SimCLR-augmentation-experiments\",\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, config[\"epochs\"]+1):  # Start epoch numbering from 1\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            h_i, h_j, z_i, z_j = model(images[0].to(device), images[1].to(device))\n",
    "            loss = criterion(z_i, z_j)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        wandb.log({\"lr\": scheduler.get_last_lr()}, step=epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(trainloader)\n",
    "        # log average loss for epoch to wandb\n",
    "        wandb.log({\"loss\": avg_loss}, step=epoch)\n",
    "        \n",
    "        print(f'Epoch {epoch} finished in {time.time() - epoch_start_time} seconds, Average Loss: {avg_loss}')\n",
    "\n",
    "    torch.save(model, f'./models/augmentation_experiments/{config[\"transform\"]}.pth')\n",
    "    wandb.finish()\n",
    "    print(\"Training completed and logs saved.\")\n",
    "    \n",
    "\n",
    "augmentations = {\n",
    "    \"random_crop\": transforms.RandomResizedCrop(32, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),\n",
    "    \"color_distortion\": get_color_distortion(),\n",
    "    \"gaussian_blur\": transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    \"shear\": transforms.RandomAffine(degrees=0, shear=30),\n",
    "    \"elastic\": transforms.ElasticTransform()\n",
    "}\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 2048,\n",
    "    \"epochs\": 200,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"projection_dim\": 128,\n",
    "    \"projection\": \"nonlinear\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"resnet\": 18,\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss\": \"NT-Xent\",\n",
    "    \"warmup_epochs\": 10,\n",
    "}\n",
    "\n",
    "for name1, transform1 in augmentations.items():\n",
    "    for name2, transform2 in augmentations.items():\n",
    "        if name1 == name2:\n",
    "            config[\"transform\"] = name1\n",
    "            run_experiment(config, transforms.Compose([\n",
    "                transform1,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ]))\n",
    "        else:\n",
    "            config[\"transform\"] = name1 + \"_and_\" + name2\n",
    "            run_experiment(config, transforms.Compose([\n",
    "                transform1,\n",
    "                transform2,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
